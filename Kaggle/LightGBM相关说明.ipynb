{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fast, distributed, high performance gradient boosting (GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.\n",
    "\n",
    "一种基于decision tree算法的快速，分布式，高性能梯度boosting框架，可用于排序，分类以及其它一些机器学习任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBDT (Gradient Boosting Decision Tree) 是机器学习中一个长盛不衰的模型，其主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。GBDT在工业界应用广泛，通常被用于点击率预测，搜索排序等任务。GBDT也是各种数据挖掘竞赛的致命武器，据统计Kaggle上的比赛有一半以上的冠军方案都是基于GBDT。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM具有以下优点：\n",
    "\n",
    "更快的训练速度\n",
    "更低的内存消耗\n",
    "更好的准确率\n",
    "分布式支持\n",
    "可以快速处理海量数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "调参\n",
    "\n",
    "1.使用num_leaves\n",
    "\n",
    "因为LightGBM使用的是leaf-wise的算法，因此在调节树的复杂程度时，使用的是num_leaves而不是max_depth\n",
    "\n",
    "大致换算关系：num_leaves = 2^(max_depth)\n",
    "\n",
    "2.对于非平衡数据集：可以param['is_unbalance']='true’\n",
    "\n",
    "3. Bagging参数：bagging_fraction+bagging_freq（必须同时设置）、feature_fraction\n",
    "\n",
    "4. min_data_in_leaf、min_sum_hessian_in_leaf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 官方文档"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- config, default=\"\", type=string, alias=config_file\n",
    "    - path of config file\n",
    "    \n",
    "- task, default=train, type=enum, options=train,prediction\n",
    "    - train for training\n",
    "    - prediction for prediction.\n",
    "    \n",
    "- application, default=regression, type=enum, options=regression,regression_l1,huber,fair,poisson,binary,lambdarank,multiclass, alias=objective,app\n",
    "    - regression, regression application\n",
    "        - regression_l2, L2 loss, alias=mean_squared_error,mse\n",
    "        - regression_l1, L1 loss, alias=mean_absolute_error,mae\n",
    "        - huber, Huber loss\n",
    "        - fair, Fair loss\n",
    "        - poisson, Poisson regression\n",
    "    - binary, binary classification application\n",
    "    - lambdarank, lambdarank application\n",
    "    - multiclass, multi-class classification application, should set num_class as well\n",
    "    \n",
    "- boosting, default=gbdt, type=enum, options=gbdt,dart, alias=boost,boosting_type\n",
    "    - gbdt, traditional Gradient Boosting Decision Tree\n",
    "    - dart, Dropouts meet Multiple Additive Regression Trees\n",
    "    - goss, Gradient-based One-Side Sampling\n",
    "    \n",
    "- data, default=\"\", type=string, alias=train,train_data\n",
    "    - training data, LightGBM will train from this data\n",
    "    \n",
    "- valid, default=\"\", type=multi-string, alias=test,valid_data,test_data\n",
    "    - validation/test data, LightGBM will output metrics for these data\n",
    "    - support multi validation data, separate by ,\n",
    "    \n",
    "- num_iterations, default=100, type=int, alias=num_iteration,num_tree,num_trees,num_round,num_rounds\n",
    "    - number of boosting iterations\n",
    "    - note: num_tree here equal with num_iterations. For multi-class, it actually learns num_class * num_iterations trees.\n",
    "    - note: For python/R package, cannot use this parameters to control number of iterations.\n",
    "    \n",
    "- learning_rate, default=0.1, type=double, alias=shrinkage_rate\n",
    "    - shrinkage rate\n",
    "    - in dart, it also affects normalization weights of dropped trees\n",
    "    \n",
    "- num_leaves, default=31, type=int, alias=num_leaf\n",
    "    - number of leaves in one tree\n",
    "    \n",
    "- tree_learner, default=serial, type=enum, options=serial,feature,data\n",
    "    - serial, single machine tree learner\n",
    "    - feature, feature parallel tree learner\n",
    "    - data, data parallel tree learner\n",
    "    - Refer to Parallel Learning Guide to get more details.\n",
    "    \n",
    "- num_threads, default=OpenMP_default, type=int, alias=num_thread,nthread\n",
    "    - Number of threads for LightGBM.\n",
    "    - For the best speed, set this to the number of real CPU cores, not the number of threads (most CPU using hyper-threading to generate 2 threads per CPU core).\n",
    "    - Do not set it too large if your dataset is small (do not use 64 threads for a dataset with 10,000 for instance).\n",
    "    - Be aware a task manager or any similar CPU monitoring tool might report cores not being fully utilized. This is normal.\n",
    "    - For parallel learning, should not use full CPU cores since this will cause poor performance for the network.\n",
    "\n",
    "- device, default=cpu, options=cpu,gpu\n",
    "    - Choose device for the tree learning, can use gpu to achieve the faster learning.\n",
    "    - Note: 1. Recommend use the smaller max_bin(e.g 63) to get the better speed up. 2. For the faster speed, GPU use 32-bit float point to sum up by default, may affect the accuracy for some tasks. You can set gpu_use_dp=true to enable 64-bit float point, but it will slow down the training. 3. Refer to Installation Guide to build with GPU ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核心参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- config, default=\"\", type=string, alias=config_file\n",
    "    - 配置文件的路径\n",
    "    \n",
    "- task, default=train, type=enum, options=train,prediction\n",
    "    - train 用来做训练\n",
    "    - prediction 用来做预测\n",
    "    \n",
    "- application, default=regression, type=enum, options=regression,regression_l1,huber,fair,poisson,binary,lambdarank,multiclass, alias=objective,app\n",
    "    - regression 用于回归\n",
    "        - regression_l2, L2损失函数, alias=mean_squared_error,mse均方误差\n",
    "        - regression_l1, L1损失函数, alias=mean_absolute_error,mae均绝对值误差\n",
    "        - huber, Huber损失函数\n",
    "        - fair, Fair损失函数\n",
    "        - poisson, Poisson回归\n",
    "    - binary, 用于二项分类\n",
    "    - lambdarank, 用于lambdarank\n",
    "    - multiclass, 用于多类别分类，需设置num_class\n",
    "    \n",
    "- boosting, default=gbdt, type=enum, options=gbdt,dart, alias=boost,boosting_type\n",
    "    - gbdt, traditional Gradient Boosting Decision Tree\n",
    "    - dart, Dropouts meet Multiple Additive Regression Trees\n",
    "    - goss, Gradient-based One-Side Sampling\n",
    "    \n",
    "- data, default=\"\", type=string, alias=train,train_data\n",
    "    - 训练集数据, 用于训练LightGBM\n",
    "    \n",
    "- valid, default=\"\", type=multi-string, alias=test,valid_data,test_data\n",
    "    - 验证集/测试集数据, LightGBM将基于该数据输出指标\n",
    "    - 支持多验证集，用“,”分开\n",
    "    \n",
    "- num_iterations, default=100, type=int, alias=num_iteration,num_tree,num_trees,num_round,num_rounds\n",
    "    - number of boosting iterations\n",
    "    - note: num_tree here equal with num_iterations. For multi-class, it actually learns num_class * num_iterations trees.\n",
    "    - note: For python/R package, cannot use this parameters to control number of iterations.\n",
    "    \n",
    "- learning_rate, default=0.1, type=double, alias=shrinkage_rate\n",
    "    - 收缩率\n",
    "    - 在dart中，它也影响所扔掉的树的正则化权重\n",
    "    \n",
    "- num_leaves, default=31, type=int, alias=num_leaf\n",
    "    - 一棵树上的叶子数目\n",
    "    \n",
    "- tree_learner, default=serial, type=enum, options=serial,feature,data\n",
    "    - serial, single machine tree learner\n",
    "    - feature, feature parallel tree learner\n",
    "    - data, data parallel tree learner\n",
    "    - Refer to Parallel Learning Guide to get more details.\n",
    "    \n",
    "- num_threads, default=OpenMP_default, type=int, alias=num_thread,nthread\n",
    "    - Number of threads for LightGBM.\n",
    "    - For the best speed, set this to the number of real CPU cores, not the number of threads (most CPU using hyper-threading to generate 2 threads per CPU core).\n",
    "    - Do not set it too large if your dataset is small (do not use 64 threads for a dataset with 10,000 for instance).\n",
    "    - Be aware a task manager or any similar CPU monitoring tool might report cores not being fully utilized. This is normal.\n",
    "    - For parallel learning, should not use full CPU cores since this will cause poor performance for the network.\n",
    "\n",
    "- device, default=cpu, options=cpu,gpu\n",
    "    - Choose device for the tree learning, can use gpu to achieve the faster learning.\n",
    "    - Note: 1. Recommend use the smaller max_bin(e.g 63) to get the better speed up. 2. For the faster speed, GPU use 32-bit float point to sum up by default, may affect the accuracy for some tasks. You can set gpu_use_dp=true to enable 64-bit float point, but it will slow down the training. 3. Refer to Installation Guide to build with GPU ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning control parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- max_depth, default=-1, type=int\n",
    "    - Limit the max depth for tree model. This is used to deal with overfit when #data is small. Tree still grow by leaf-wise.\n",
    "    - < 0 means no limit\n",
    "    \n",
    "- min_data_in_leaf, default=20, type=int, alias=min_data_per_leaf , min_data\n",
    "    - Minimal number of data in one leaf. Can use this to deal with over-fit.\n",
    "    \n",
    "- min_sum_hessian_in_leaf, default=1e-3, type=double, alias=min_sum_hessian_per_leaf, min_sum_hessian, min_hessian\n",
    "    - Minimal sum hessian in one leaf. Like min_data_in_leaf, can use this to deal with over-fit.\n",
    "\n",
    "- feature_fraction, default=1.0, type=double, 0.0 < feature_fraction < 1.0, alias=sub_feature\n",
    "    - LightGBM will random select part of features on each iteration if feature_fraction smaller than 1.0. For example, if set to 0.8, will select 80% features before training each tree.\n",
    "    - Can use this to speed up training\n",
    "    - Can use this to deal with over-fit\n",
    "\n",
    "- feature_fraction_seed, default=2, type=int\n",
    "    - Random seed for feature fraction.\n",
    "\n",
    "- bagging_fraction, default=1.0, type=double, , 0.0 < bagging_fraction < 1.0, alias=sub_row\n",
    "    - Like feature_fraction, but this will random select part of data\n",
    "    - Can use this to speed up training\n",
    "    - Can use this to deal with over-fit\n",
    "    - Note: To enable bagging, should set bagging_freq to a non zero value as well\n",
    "\n",
    "- bagging_freq, default=0, type=int\n",
    "    - Frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration.\n",
    "    - Note: To enable bagging, should set bagging_fraction as well\n",
    "\n",
    "- bagging_seed , default=3, type=int\n",
    "    - Random seed for bagging.\n",
    "\n",
    "- early_stopping_round , default=0, type=int, alias=early_stopping_rounds,early_stopping\n",
    "    - Will stop training if one metric of one validation data doesn't improve in last early_stopping_round rounds.\n",
    "\n",
    "- lambda_l1 , default=0, type=double\n",
    "    - l1 regularization\n",
    "\n",
    "- lambda_l2 , default=0, type=double\n",
    "    - l2 regularization\n",
    "\n",
    "- min_gain_to_split , default=0, type=double\n",
    "    - The minimal gain to perform split\n",
    "\n",
    "- drop_rate, default=0.1, type=double\n",
    "    - only used in dart\n",
    "\n",
    "- skip_drop, default=0.5, type=double\n",
    "    - only used in dart, probability of skipping drop\n",
    "\n",
    "- max_drop, default=50, type=int\n",
    "    - only used in dart, max number of dropped trees on one iteration. <=0 means no limit.\n",
    "\n",
    "- uniform_drop, default=false, type=bool\n",
    "    - only used in dart, true if want to use uniform drop\n",
    "\n",
    "- xgboost_dart_mode, default=false, type=bool\n",
    "    - only used in dart, true if want to use xgboost dart mode\n",
    "\n",
    "- drop_seed, default=4, type=int\n",
    "    - only used in dart, used to random seed to choose dropping models.\n",
    "\n",
    "- top_rate, default=0.2, type=double\n",
    "    - only used in goss, the retain ratio of large gradient data\n",
    "\n",
    "- other_rate, default=0.1, type=int\n",
    "    - only used in goss, the retain ratio of small gradient data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习控制参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- max_depth, default=-1, type=int\n",
    "    - Limit the max depth for tree model. This is used to deal with overfit when #data is small. Tree still grow by leaf-wise.\n",
    "    - < 0 means no limit\n",
    "    \n",
    "- min_data_in_leaf, default=20, type=int, alias=min_data_per_leaf , min_data\n",
    "    - Minimal number of data in one leaf. Can use this to deal with over-fit.\n",
    "    \n",
    "- min_sum_hessian_in_leaf, default=1e-3, type=double, alias=min_sum_hessian_per_leaf, min_sum_hessian, min_hessian\n",
    "    - Minimal sum hessian in one leaf. Like min_data_in_leaf, can use this to deal with over-fit.\n",
    "\n",
    "- feature_fraction, default=1.0, type=double, 0.0 < feature_fraction < 1.0, alias=sub_feature\n",
    "    - 如果feature_fraction小于1.0，LightGBM将在每轮迭代中随机选择部分特征，如果设置为0.8，将在训练每棵树前选择80%的特征\n",
    "    - 可以用这个参数加速训练\n",
    "    - 可以用这个参数处理过拟合\n",
    "\n",
    "- feature_fraction_seed, default=2, type=int\n",
    "    - Random seed for feature fraction.\n",
    "\n",
    "- bagging_fraction, default=1.0, type=double, , 0.0 < bagging_fraction < 1.0, alias=sub_row\n",
    "    - 与feature_fraction类似, 但是这个参数将随机选择部分数据\n",
    "    - 可以用这个参数加速训练\n",
    "    - 可以用这个参数处理过拟合\n",
    "    - 注意：要激活bagging,需设置bagging_freq为一个非零的值\n",
    "\n",
    "- bagging_freq, default=0, type=int\n",
    "    - Frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration.\n",
    "    - Note: To enable bagging, should set bagging_fraction as well\n",
    "\n",
    "- bagging_seed , default=3, type=int\n",
    "    - Random seed for bagging.\n",
    "\n",
    "- early_stopping_round , default=0, type=int, alias=early_stopping_rounds,early_stopping\n",
    "    - Will stop training if one metric of one validation data doesn't improve in last early_stopping_round rounds.\n",
    "\n",
    "- lambda_l1 , default=0, type=double\n",
    "    - l1 regularization\n",
    "\n",
    "- lambda_l2 , default=0, type=double\n",
    "    - l2 regularization\n",
    "\n",
    "- min_gain_to_split , default=0, type=double\n",
    "    - The minimal gain to perform split\n",
    "\n",
    "- drop_rate, default=0.1, type=double\n",
    "    - only used in dart\n",
    "\n",
    "- skip_drop, default=0.5, type=double\n",
    "    - only used in dart, probability of skipping drop\n",
    "\n",
    "- max_drop, default=50, type=int\n",
    "    - only used in dart, max number of dropped trees on one iteration. <=0 means no limit.\n",
    "\n",
    "- uniform_drop, default=false, type=bool\n",
    "    - only used in dart, true if want to use uniform drop\n",
    "\n",
    "- xgboost_dart_mode, default=false, type=bool\n",
    "    - only used in dart, true if want to use xgboost dart mode\n",
    "\n",
    "- drop_seed, default=4, type=int\n",
    "    - only used in dart, used to random seed to choose dropping models.\n",
    "\n",
    "- top_rate, default=0.2, type=double\n",
    "    - only used in goss, the retain ratio of large gradient data\n",
    "\n",
    "- other_rate, default=0.1, type=int\n",
    "    - only used in goss, the retain ratio of small gradient data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert parameters from XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM uses leaf-wise tree growth algorithm. But other popular tools, e.g. XGBoost, use depth-wise tree growth. So LightGBM use num_leaves to control complexity of tree model, and other tools usually use max_depth. Following table is the correspond between leaves and depths. The relation is num_leaves = 2^(max_depth)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For faster speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use bagging by set bagging_fraction and bagging_freq\n",
    "- Use feature sub-sampling by set feature_fraction\n",
    "- Use small max_bin\n",
    "- Use save_binary to speed up data loading in future learning\n",
    "- Use parallel learning, refer to parallel learning guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For better accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use large max_bin (may be slower)\n",
    "- Use small learning_rate with large num_iterations\n",
    "- Use large num_leaves(may cause over-fitting)\n",
    "- Use bigger training data\n",
    "- Try dart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with over-fitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use small max_bin\n",
    "- Use small num_leaves\n",
    "- Use min_data_in_leaf and min_sum_hessian_in_leaf\n",
    "- Use bagging by set bagging_fraction and bagging_freq\n",
    "- Use feature sub-sampling by set feature_fraction\n",
    "- Use bigger training data\n",
    "- Try lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "- Try max_depth to avoid growing deep tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
