{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONGO=\"mongodb://app_reportsdb:h8sgk6RjALqKzJm@nosql05.wmcloud-dev.com/reports_db\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGO=\"mongodb://app_reports_db_rw:3zqgzqvKgvRfOa3K@mongodb01.wmcloud-stg.com,mongodb02.wmcloud-stg.com,mongodb03.wmcloud-stg.com/reports_db?readPreference=secondaryPreferred\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGO=\"mongodb://app_reports_db_rw:OxXtSsFeZX0JZEZk@mongodb01-dbp.datayes.com,mongodb02-dbp.datayes.com,mongodb03-dbp.datayes.com/reports_db?readPreference=secondaryPreferred\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGO=\"mongodb://app_reportsdb:h8sgk6RjALqKzJm@nosql05.wmcloud-dev.com/reports_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "mongo_conn = pymongo.MongoClient(MONGO, connect=False)\n",
    "mongo_db = mongo_conn.get_database(\"reports_db\")\n",
    "\n",
    "print(pymongo.__version__)\n",
    "collection='report_html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = \"processTimePretty\"\n",
    "value = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# \"finishTimePretty\" : \"2018-08-03 15:37:17\"\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskStatus():\n",
    "    SUBMIT = \"undo\"\n",
    "    ALERT = \"emergency\" # 紧急任务，可以插队的那种\n",
    "    TRANSFERING = \"converting\"  # pdf -> html\n",
    "    CONVERTED = \"converted\"  #\n",
    "    UPLOADING = \"uploading\"  #\n",
    "    FINISH = \"uploaded\"  #\n",
    "    PASSUP = \"passupload\"  #\n",
    "    SYNC = \"synced\"\n",
    "    INFORMED = \"informed\"  #\n",
    "    DONE = \"done\"  #\n",
    "    FAIL = 'fail'\n",
    "    SLEEP = 'sleep'\n",
    "    REDO = 'redo'  # 当只重做转换，而不需要上传S3跟通知下游时， redo线程搜寻此状态重做转换，完成后状态直接置为done.\n",
    "    STATUSFALSE = 'false'\n",
    "    STATUSTRUE = 'true'\n",
    "taskstatus = TaskStatus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'_id': ObjectId('5e0491ab3a768246c39e7545'),\n",
       " u'converting': 1577357744.754,\n",
       " u'insertTime': u'2019-12-26 18:55:39',\n",
       " u'machine': u'10.22.220.181',\n",
       " u'message': u'',\n",
       " u'pdfSize': 159862L,\n",
       " u'processTime': 1577357747.944,\n",
       " u'progress': u'converted',\n",
       " u'publishDate': u'2019-12-27',\n",
       " u'receiveTime': u'2019-12-26 18:55:39',\n",
       " u'redo_convert': False,\n",
       " u'reportId': 30458224,\n",
       " u'report_type': 1,\n",
       " u's3_address': u'http://cluster-s3nginx-inner.datayes.com:80/pipeline/report/2019-12-27/20191227_1664f2009b1d5172ca2851f5e639a82bb2d9f5c9f.pdf',\n",
       " u'site': u'sz',\n",
       " u'status': True,\n",
       " u'taskId': u'8905a1b8f548a1079109b40d7507611d',\n",
       " u'title': u'\\u987a\\u704f\\u80a1\\u4efd:\\u5173\\u4e8e\\u5458\\u5de5\\u6301\\u80a1\\u8ba1\\u5212\\u5b8c\\u6210\\u975e\\u4ea4\\u6613\\u8fc7\\u6237\\u7684\\u516c\\u544a',\n",
       " u'tool': u'datayes_api',\n",
       " u'tried_count': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongo_db.get_collection(collection).find_one({\"reportId\":{\"$in\":[30458224]}})\n",
    "# mongo_db.get_collection(collection).find_one({\"progress\":{\"$in\":['passupload']}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 限定一个入库时间段的不同状态的统计，不然不太好判断进度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# upload容易挂， count('converted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "32\n",
      "69\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(mongo_db.get_collection(collection).find({'progress':'undo'}).count())\n",
    "print(mongo_db.get_collection(collection).find({'progress':'converted'}).count())\n",
    "print(mongo_db.get_collection(collection).find({'progress':'converting'}).count())\n",
    "print(mongo_db.get_collection(collection).find({'progress':'uploaded'}).count())\n",
    "print(mongo_db.get_collection(collection).find({'progress':'synced'}).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 库中所有任务统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>progress</th>\n",
       "      <th>status</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>converted</td>\n",
       "      <td>False</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>converted</td>\n",
       "      <td>True</td>\n",
       "      <td>2866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>done</td>\n",
       "      <td>False</td>\n",
       "      <td>337998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>done</td>\n",
       "      <td>True</td>\n",
       "      <td>1000807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synced</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>uploaded</td>\n",
       "      <td>False</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>uploaded</td>\n",
       "      <td>True</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>uploading</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    progress  status        0\n",
       "0  converted   False      150\n",
       "1  converted    True     2866\n",
       "2       done   False   337998\n",
       "3       done    True  1000807\n",
       "4     synced    True       13\n",
       "5   uploaded   False       25\n",
       "6   uploaded    True       35\n",
       "7  uploading    True       10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publishdate = str(datetime.today().date())\n",
    "result = {}\n",
    "explode = []\n",
    "res = mongo_db.get_collection(collection).find({})\n",
    "df=pd.DataFrame(list(res))\n",
    "df.groupby(['progress','status']).size().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publishdate = str(datetime.today().date())\n",
    "result = {}\n",
    "explode = []\n",
    "res = mongo_db.get_collection(collection).find({'insertTime':{\"$exists\":1}})\n",
    "df=pd.DataFrame(list(res))\n",
    "df.groupby(['progress','status']).size().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>progress</th>\n",
       "      <th>status</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>converted</td>\n",
       "      <td>False</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>converting</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>passupload</td>\n",
       "      <td>False</td>\n",
       "      <td>204474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>synced</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synced</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>undo</td>\n",
       "      <td>False</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>uploaded</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>uploaded</td>\n",
       "      <td>True</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     progress  status       0\n",
       "0   converted   False      15\n",
       "1  converting   False       4\n",
       "2  passupload   False  204474\n",
       "3      synced   False       1\n",
       "4      synced    True       2\n",
       "5        undo   False     203\n",
       "6    uploaded   False       1\n",
       "7    uploaded    True     489"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publishdate = str(datetime.today().date())\n",
    "result = {}\n",
    "explode = []\n",
    "res = mongo_db.get_collection(collection).find({\"progress\":{\"$in\":keys[1:]},'insertTime':{\"$exists\":1}})\n",
    "df=pd.DataFrame(list(res))\n",
    "df.groupby(['progress','status']).size().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计图示 按insertTime, progress统计转换完成进度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['insertTime'] = pd.to_datetime(df['insertTime'])\n",
    "\n",
    "# df.head()\n",
    "# # df['insert_hour'] = df['insertTime'].apply(lambda x: str(x)[:13])\n",
    "# df['timeflag'] = df['insertTime'].apply(lambda x: str(x)[:13])\n",
    "# groupdf = df.groupby(['timeflag','progress']).size().reset_index()\n",
    "# # groupdf[groupdf.progress != 'done']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# processTime 最近七天处理情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print publishdate\n",
    "import time\n",
    "# result = {}\n",
    "data = []\n",
    "result = mongo_db.get_collection(collection).find({\"processTime\": {\"$gt\": time.time()-24*3600*7}})\n",
    "curday = pd.DataFrame(list(result))\n",
    "# curday['proTime'] = pd.to_datetime(curday['processTime']).dt.strftime(\"%Y-%m-%d\")\n",
    "# curday['timeflag'] = curday['insertTime'].apply(lambda x: str(x)[:13])\n",
    "# groupdf = curday.groupby(['progress']).size().reset_index()\n",
    "# groupdf[groupdf.progress != 'done']\n",
    "# groupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2019-10-28 03:06:43')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# curday.head()\n",
    "datetime.fromtimestamp(1572232003)\n",
    "pd.Timestamp(1572232003,unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "curday['processTime'] = curday['processTime'].astype(int)\n",
    "curday['processTime'] = curday['processTime'].apply(lambda x: pd.Timestamp(x, unit='s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curday['processTime'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curday['processTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>progress</th>\n",
       "      <th>proTime</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>converted</td>\n",
       "      <td>2019-10-28</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>done</td>\n",
       "      <td>2019-10-28</td>\n",
       "      <td>3278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>undo</td>\n",
       "      <td>2019-10-28</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>passupload</td>\n",
       "      <td>2019-10-28</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>converting</td>\n",
       "      <td>2019-10-28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>undo</td>\n",
       "      <td>2019-10-27</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>done</td>\n",
       "      <td>2019-10-27</td>\n",
       "      <td>24266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>done</td>\n",
       "      <td>2019-10-26</td>\n",
       "      <td>23052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>done</td>\n",
       "      <td>2019-10-25</td>\n",
       "      <td>13753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>uploaded</td>\n",
       "      <td>2019-10-25</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sleep</td>\n",
       "      <td>2019-10-25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>undo</td>\n",
       "      <td>2019-10-25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>done</td>\n",
       "      <td>2019-10-24</td>\n",
       "      <td>6025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>uploaded</td>\n",
       "      <td>2019-10-24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>undo</td>\n",
       "      <td>2019-10-24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sleep</td>\n",
       "      <td>2019-10-23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>uploaded</td>\n",
       "      <td>2019-10-23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>uploading</td>\n",
       "      <td>2019-10-23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>done</td>\n",
       "      <td>2019-10-23</td>\n",
       "      <td>6271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>converting</td>\n",
       "      <td>2019-10-23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>done</td>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>6365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sleep</td>\n",
       "      <td>2019-10-21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>done</td>\n",
       "      <td>2019-10-21</td>\n",
       "      <td>6003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      progress     proTime      0\n",
       "0    converted  2019-10-28     12\n",
       "10        done  2019-10-28   3278\n",
       "18        undo  2019-10-28     61\n",
       "11  passupload  2019-10-28     50\n",
       "2   converting  2019-10-28      2\n",
       "17        undo  2019-10-27      3\n",
       "9         done  2019-10-27  24266\n",
       "8         done  2019-10-26  23052\n",
       "7         done  2019-10-25  13753\n",
       "21    uploaded  2019-10-25    257\n",
       "14       sleep  2019-10-25      1\n",
       "16        undo  2019-10-25      1\n",
       "6         done  2019-10-24   6025\n",
       "20    uploaded  2019-10-24      1\n",
       "15        undo  2019-10-24      2\n",
       "13       sleep  2019-10-23      1\n",
       "19    uploaded  2019-10-23      2\n",
       "22   uploading  2019-10-23      1\n",
       "5         done  2019-10-23   6271\n",
       "1   converting  2019-10-23      1\n",
       "4         done  2019-10-22   6365\n",
       "12       sleep  2019-10-21      1\n",
       "3         done  2019-10-21   6003"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curday['proTime'] = pd.to_datetime(curday['processTime']).dt.strftime(\"%Y-%m-%d\")\n",
    "curday['timeflag'] = curday['insertTime'].apply(lambda x: str(x)[:13])\n",
    "groupdf = curday.groupby(['progress','proTime']).size().reset_index().sort_values('proTime',ascending=False)\n",
    "# groupdf[groupdf.progress != 'done']\n",
    "groupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# processTime 最近一天处理情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'publishdate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-494fe33a0729>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mpublishdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# result = {}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmongo_db\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"processTime\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"$gt\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m3600\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'publishdate' is not defined"
     ]
    }
   ],
   "source": [
    "print publishdate\n",
    "import time\n",
    "# result = {}\n",
    "data = []\n",
    "result = mongo_db.get_collection(collection).find({\"processTime\": {\"$gt\": time.time()-24*3600}})\n",
    "curday = pd.DataFrame(list(result))\n",
    "# curday['insertTime'] = pd.to_datetime(curday['insertTime'])\n",
    "# curday['timeflag'] = curday['insertTime'].apply(lambda x: str(x)[:13])\n",
    "groupdf = curday.groupby(['progress']).size().reset_index()\n",
    "# groupdf[groupdf.progress != 'done']\n",
    "groupdf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 当天入库的任务， insertTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeflag</th>\n",
       "      <th>progress</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2019-10-24 17</td>\n",
       "      <td>undo</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2019-10-24 17</td>\n",
       "      <td>done</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2019-10-24 17</td>\n",
       "      <td>converting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2019-10-24 17</td>\n",
       "      <td>converted</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019-10-24 16</td>\n",
       "      <td>uploaded</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-10-24 16</td>\n",
       "      <td>undo</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-10-24 16</td>\n",
       "      <td>synced</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-10-24 16</td>\n",
       "      <td>passupload</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-10-24 16</td>\n",
       "      <td>done</td>\n",
       "      <td>1227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-10-24 16</td>\n",
       "      <td>converted</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-10-24 15</td>\n",
       "      <td>done</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-10-24 14</td>\n",
       "      <td>done</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-10-24 13</td>\n",
       "      <td>uploaded</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-10-24 13</td>\n",
       "      <td>done</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-10-24 12</td>\n",
       "      <td>done</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-10-24 11</td>\n",
       "      <td>done</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-10-24 10</td>\n",
       "      <td>done</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-10-24 09</td>\n",
       "      <td>done</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-10-24 08</td>\n",
       "      <td>done</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-24 07</td>\n",
       "      <td>done</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-24 03</td>\n",
       "      <td>done</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-24 02</td>\n",
       "      <td>done</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-24 01</td>\n",
       "      <td>done</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-24 00</td>\n",
       "      <td>done</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         timeflag    progress     0\n",
       "23  2019-10-24 17        undo    10\n",
       "22  2019-10-24 17        done   114\n",
       "21  2019-10-24 17  converting     1\n",
       "20  2019-10-24 17   converted    75\n",
       "19  2019-10-24 16    uploaded   283\n",
       "18  2019-10-24 16        undo    10\n",
       "17  2019-10-24 16      synced     6\n",
       "16  2019-10-24 16  passupload     4\n",
       "15  2019-10-24 16        done  1227\n",
       "14  2019-10-24 16   converted   167\n",
       "13  2019-10-24 15        done   685\n",
       "12  2019-10-24 14        done    98\n",
       "11  2019-10-24 13    uploaded     1\n",
       "10  2019-10-24 13        done   235\n",
       "9   2019-10-24 12        done   283\n",
       "8   2019-10-24 11        done    72\n",
       "7   2019-10-24 10        done    81\n",
       "6   2019-10-24 09        done   152\n",
       "5   2019-10-24 08        done   430\n",
       "4   2019-10-24 07        done     1\n",
       "3   2019-10-24 03        done   101\n",
       "2   2019-10-24 02        done     1\n",
       "1   2019-10-24 01        done     1\n",
       "0   2019-10-24 00        done    49"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publishdate = str(datetime.today().date())\n",
    "# result = {}\n",
    "data = []\n",
    "result = mongo_db.get_collection(collection).find({\"insertTime\": {\"$regex\": publishdate}})\n",
    "curday = pd.DataFrame(list(result))\n",
    "curday['insertTime'] = pd.to_datetime(curday['insertTime'])\n",
    "curday['timeflag'] = curday['insertTime'].apply(lambda x: str(x)[:13])\n",
    "groupdf = curday.groupby(['timeflag','progress']).size().reset_index()\n",
    "# groupdf[groupdf.progress != 'done']\n",
    "groupdf.sort_values('timeflag', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前一天入库的任务， insertTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-17\n",
      "(u'converted_b', 0)\n",
      "(u'uploading_stuck', 0)\n",
      "(u'done', 0)\n",
      "(u'sleep', 0)\n",
      "(u'uploading', 0)\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "publishdate = str((datetime.today()-timedelta(days=1)).date())\n",
    "print publishdate\n",
    "result = {}\n",
    "explode = []\n",
    "for k in keys:\n",
    "    count = mongo_db.get_collection(collection).find({\"progress\" : k, \"insertTime\": {\"$regex\": publishdate}}).count()\n",
    "    result[k] = count\n",
    "    print(k, count)\n",
    "    explode.append(0)\n",
    "\n",
    "data = []\n",
    "result = mongo_db.get_collection(collection).find({\"insertTime\": {\"$regex\": publishdate}})\n",
    "for item in result:\n",
    "#     print(item)\n",
    "    data.append(item)\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{u'_id': ObjectId('5d716d40d52b37bdc54643c5'),\n",
       " u'converting': 1567801616.382,\n",
       " u'insertTime': u'2019-09-06 04:17:04',\n",
       " u'machine': u'10.22.161.83',\n",
       " u'message': u'',\n",
       " u'processTime': 1567801621.488,\n",
       " u'progress': u'converted',\n",
       " u'publishDate': u'2019-08-22',\n",
       " u'receiveTime': u'2019-09-06 04:17:04',\n",
       " u'redo_convert': False,\n",
       " u'reportId': 26938725,\n",
       " u'report_type': 1,\n",
       " u's3_address': u'http://cluster-s3nginx-inner.datayes-stg.com:80/pipeline/report/2019-08-22/20190822_16fd14c3fb774c47df58e762ea5ed71d0968e4c99.pdf',\n",
       " u'site': u'sh',\n",
       " u'status': True,\n",
       " u'taskId': u'39417bf709e328a10987c8ff0e300044',\n",
       " u'title': u'\\u6c5f\\u82cf\\u5e38\\u719f\\u519c\\u6751\\u5546\\u4e1a\\u94f6\\u884c\\u80a1\\u4efd\\u6709\\u9650\\u516c\\u53f8\\u6295\\u8d44\\u8005\\u8c03\\u7814\\u7eaa\\u8981',\n",
       " u'tool': u'datayes_api',\n",
       " u'tried_count': 1}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "publishdate = str((datetime.today()-timedelta(days=1)).date())\n",
    "print publishdate\n",
    "mongo_db.get_collection(collection).find_one({\"insertTime\": {\"$regex\": publishdate},\"progress\" : 'converted'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress \n",
    "progress = df[['progress']].groupby('progress').size()\n",
    "keys = progress.keys()\n",
    "# progress.values\n",
    "# plt.bar(progress.keys(), progress.values)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 指定发布日期的统计结果 publish date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeflag</th>\n",
       "      <th>progress</th>\n",
       "      <th>status</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-08-27 00</td>\n",
       "      <td>done</td>\n",
       "      <td>False</td>\n",
       "      <td>1190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-08-27 00</td>\n",
       "      <td>done</td>\n",
       "      <td>True</td>\n",
       "      <td>9532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        timeflag progress  status     0\n",
       "0  2019-08-27 00     done   False  1190\n",
       "1  2019-08-27 00     done    True  9532"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publishdate = str(datetime.today().date())\n",
    "print publishdate\n",
    "# result = {}\n",
    "data = []\n",
    "result = mongo_db.get_collection(collection).find({\"publishDate\": '2019-08-27'})\n",
    "curday = pd.DataFrame(list(result))\n",
    "curday['publishDate'] = pd.to_datetime(curday['publishDate'])\n",
    "curday['timeflag'] = curday['publishDate'].apply(lambda x: str(x)[:13])\n",
    "groupdf = curday.groupby(['timeflag','progress','status']).size().reset_index()\n",
    "# groupdf[groupdf.progress != 'done']\n",
    "groupdf\n",
    "\n",
    "# df.groupby(['progress','status']).size().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO 关注一下这个统计结果， 看看fail的问题有没有异常多， 因为添加了是否是空htm的判断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 发布日期为当天的运行结果 publish date is today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-17\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'publishDate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-f82111d02606>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmongo_db\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"publishDate\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpublishdate\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcurday\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcurday\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'publishDate'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurday\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'publishDate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mcurday\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'timeflag'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurday\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'publishDate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mgroupdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurday\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'timeflag'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'progress'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2137\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2138\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2145\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2146\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2148\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1842\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3842\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3843\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3844\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pandas\\core\\indexes\\base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2525\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2527\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'publishDate'"
     ]
    }
   ],
   "source": [
    "publishdate = str(datetime.today().date())\n",
    "print publishdate\n",
    "# result = {}\n",
    "data = []\n",
    "result = mongo_db.get_collection(collection).find({\"publishDate\": publishdate})\n",
    "curday = pd.DataFrame(list(result))\n",
    "curday['publishDate'] = pd.to_datetime(curday['publishDate'])\n",
    "curday['timeflag'] = curday['publishDate'].apply(lambda x: str(x)[:13])\n",
    "groupdf = curday.groupby(['timeflag','progress']).size().reset_index()\n",
    "# groupdf[groupdf.progress != 'done']\n",
    "groupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONGO 发布日期为明天的运行结果 publish date is tomorrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-18\n",
      "(u'done', 788)\n",
      "(u'sleep', 0)\n",
      "(u'converting', 0)\n",
      "(u'uploading', 0)\n",
      "(u'undo', 0)\n",
      "(u'synced', 0)\n",
      "(u'uploaded', 0)\n",
      "(u'converted', 23)\n"
     ]
    }
   ],
   "source": [
    "publishdate = str((datetime.today()+timedelta(days=1)).date())\n",
    "print publishdate\n",
    "\n",
    "result = {}\n",
    "explode = []\n",
    "for k in keys:\n",
    "    count = mongo_db.get_collection(collection).find({\"progress\" : k, \"publishDate\": publishdate}).count()\n",
    "    result[k] = count\n",
    "    print(k, count)\n",
    "    explode.append(0)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONGO实时运行情况统计结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the latest 24 Hours , report_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timestamp = time.time()\n",
    "stime = timestamp - 24*3600\n",
    "# print stime\n",
    "result = {}\n",
    "# keys = [\"queueing\", \"doing\", \"undo\"]\n",
    "types=[0,1,2,3]\n",
    "# keys = [\"undo\", \"converting\", \"converted\", \"uploaded\", \"uploading\",\"synced\"]\n",
    "for k in types:\n",
    "    count = mongo_db.get_collection(collection).find({\"report_type\" : k,\n",
    "                                     \"processTime\" : {'$gt': stime}}).count()\n",
    "    # TODO how to group \n",
    "\n",
    "    result[k] = count\n",
    "    print(k, count)\n",
    "# result\n",
    "# print (\"undo: %s, queueing: %b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timestamp = time.time()\n",
    "stime = timestamp - 8*3600\n",
    "# print stime\n",
    "result = {}\n",
    "# keys = [\"queueing\", \"doing\", \"undo\"]\n",
    "types=[0,1,2,3]\n",
    "# keys = [\"undo\", \"converting\", \"converted\", \"uploaded\", \"uploading\",\"synced\"]\n",
    "for k in types:\n",
    "    count = mongo_db.get_collection(collection).find({\"report_type\" : k,\n",
    "                                     \"processTime\" : {'$gt': stime}}).count()\n",
    "    # TODO how to group \n",
    "\n",
    "    result[k] = count\n",
    "    print(k, count)\n",
    "result\n",
    "# print (\"undo: %s, queueing: %b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the latest 24 Hours , progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>progress</th>\n",
       "      <th>status</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>converted</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>converting</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>done</td>\n",
       "      <td>False</td>\n",
       "      <td>898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>done</td>\n",
       "      <td>True</td>\n",
       "      <td>1752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synced</td>\n",
       "      <td>False</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>uploaded</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>uploaded</td>\n",
       "      <td>True</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     progress  status     0\n",
       "0   converted   False     5\n",
       "1  converting   False     1\n",
       "2        done   False   898\n",
       "3        done    True  1752\n",
       "4      synced   False    13\n",
       "5    uploaded   False     1\n",
       "6    uploaded    True   162"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publishdate = str(datetime.today().date())\n",
    "print publishdate\n",
    "# result = {}\n",
    "import time\n",
    "timestamp = time.time()\n",
    "stime = timestamp - 24*3600\n",
    "data = []\n",
    "result = mongo_db.get_collection(collection).find({\"processTime\" : {'$gt': stime}})\n",
    "curday = pd.DataFrame(list(result))\n",
    "# curday['publishDate'] = pd.to_datetime(curday['publishDate'])\n",
    "# curday['timeflag'] = curday['publishDate'].apply(lambda x: str(x)[:13])\n",
    "groupdf = curday.groupby(['progress','status']).size().reset_index()\n",
    "# groupdf[groupdf.progress != 'done']\n",
    "groupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timestamp = time.time()\n",
    "stime = timestamp - 24*3600\n",
    "# print stime\n",
    "result = {}\n",
    "# keys = [\"queueing\", \"doing\", \"undo\"]\n",
    "# keys = [\"undo\", \"converting\", \"converted\", \"uploaded\", \"uploading\",\"synced\"]\n",
    "for k in keys:\n",
    "    count = mongo_db.get_collection(collection).find({\"progress\" : k,\n",
    "                                     \"processTime\" : {'$gt': stime}}).count()\n",
    "    # TODO how to group \n",
    "#     mongo_db.get_collection(collection).aggregate(\n",
    "#     {\"$group\" : { \"publishDate\": \"$publishDate\", \n",
    "#                  \"count\": { \"$sum\": 1 } } }\n",
    "# )\n",
    "\n",
    "    result[k] = count\n",
    "    print(k, count)\n",
    "result\n",
    "# print (\"undo: %s, queueing: %b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%%s%s' % 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看状态为Undo 的任务的publish date 分布计数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in mongo_db.get_collection(collection).aggregate([\n",
    "    {'$match': {\"progress\" : 'undo'}},\n",
    "    {\"$group\" : { \"_id\": \"$publishDate\", \"count\": {\"$sum\": 1 } } },\n",
    "    {\"$sort\" :{\"_id\": -1}}\n",
    "    ]):\n",
    "    print(doc)\n",
    "# .toArray()#.forEach(function(x) { print(x._id + x._count);})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 转换速度统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting, 到最后processtime之前的差值， 需要转换 过去6小时 TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for doc in mongo_db.get_collection(collection).aggregate([\n",
    "    {'$match': {\"progress\" : 'undo'}},\n",
    "    {\"$group\" : { \"_id\": \"$publishDate\", \"count\": {\"$sum\": 1 } } },\n",
    "    {\"$sort\" :{\"_id\": -1}}\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timestamp = time.time()\n",
    "stime = timestamp - 6*3600\n",
    "# print stime\n",
    "mongo_db.get_collection(collection).aggregate([\n",
    "    {'$match': {\"progress\" : 'done', \"processTime\" : {'$gt': stime}}},\n",
    "    {'$project': },                 \n",
    "    {\"$sort\" :{\"_id\": -1}}\n",
    "    ]):\n",
    "    # TODO how to group \n",
    "#     mongo_db.get_collection(collection).aggregate(\n",
    "#     {\"$group\" : { \"publishDate\": \"$publishDate\", \n",
    "#                  \"count\": { \"$sum\": 1 } } }\n",
    "# )\n",
    "\n",
    "    result[k] = count\n",
    "    print(k, count)\n",
    "result\n",
    "# print (\"undo: %s, queueing: %s, doing: %s\" % (result[\"undo\"], result[\"queueing\"], result[\"doing\"]))\n",
    "\n",
    "# plt.pie(result.values() , explode=explode,  labels=result.keys()  ,autopct = '%3.1f%%', shadow=True)\n",
    "\n",
    "# plt.axis('equal')\n",
    "# plt.legend()\n",
    "# # plt.title(\"pubDate = %s\" % value)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# undo = mongo_db.solid_pdfs.find({\"progress\" : \"undo\", key:{\"$gt\": value}}).count()\n",
    "\n",
    "# queueing= mongo_db.solid_pdfs.find({\"progress\" : \"queueing\", key:{\"$gt\": value}}).count()\n",
    "\n",
    "# doing= mongo_db.solid_pdfs.find({\"progress\" : \"doing\", key:{\"$gt\": value}}).count()\n",
    "\n",
    "# done= mongo_db.rpt_earnings_gen.find({\"progress\" : \"done\", key:{\"$gt\": value}}).count()\n",
    "\n",
    "# print \"undo=%s, queueing=%s, doing=%s, done=%s\" % (undo, queueing, doing, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "explode = (0, 0.1, 0, 0)\n",
    "\n",
    "plt.pie(result.values() , explode=explode,  labels=result.keys()  ,autopct = '%3.1f%%', shadow=True)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "plt.title(\"pubDate = %s\" % value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MysqlConn(object):\n",
    "    def __init__(self, config_json_str):\n",
    "        # self.mysql_config = {\"host\": \"10.22.128.150\",  \"port\": 3317, \"db\": \"bigdata\", \"user\": \"talend_load\",\n",
    "        #   \"passwd\": \"s9t5gNThn2vqWM7c\" , \"charset\" : \"utf8\"}\n",
    "        # self.mysql_config = {\"host\": \"db-bigdata.wmcloud-qa.com\",  \"port\": 3312, \"db\": \"bigdata\", \"user\": \"app_bigdata_ro\",\n",
    "        #   \"passwd\": \"Welcome_20141217\"}\n",
    "        self.mysql_config = json.loads(config_json_str, encoding='utf-8')\n",
    "\n",
    "    def connect(self):\n",
    "        mysql_conn = pymysql.connect(**self.mysql_config)\n",
    "        return mysql_conn\n",
    "\n",
    "class MssqlConn(object):\n",
    "    def __init__(self, config_json_str):\n",
    "        # self.mssql_config = {\"server\": \"sh-datayesdb.wmcloud-dev.com\",  \"port\": 1433, \"database\": \"datayesdb\",\n",
    "        #     \"user\": \"talend_load\", \"password\": \"Welcome01\"}\n",
    "        self.mssql_config = json.loads(config_json_str, encoding='utf-8')\n",
    "\n",
    "    def connect(self):\n",
    "        mssql_conn = pymssql.connect(**self.mssql_config)\n",
    "        return mssql_conn\n",
    "\n",
    "    def __str__(self):\n",
    "        return json.dumps(self.mssql_config, ensure_ascii=False, encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pymysql\n",
    "import pymssql\n",
    "import pandas as pd\n",
    "# MySQL\n",
    "# bigdata={\"host\":\"security03-dev.datayes.com\",\"port\":3306,\"user\":\"talend_load\",\"passwd\":\"NCph1G9BQT3DuQj\",\"db\":\"bigdata\",\"charset\":\"utf8\"}\n",
    "# dateyesdbp\n",
    "bigdata_conn={\"host\":\"db-bigdata.wmcloud.com\",\"port\":3317,\"user\":\"talend_load\",\"passwd\":\"s9t5gNThn2vqWM7c\",\"db\":\"bigdata\",\"charset\":\"utf8\"}\n",
    "\n",
    "bigdata_conn={\"host\":\"db-bigdata-ro.wmcloud.com\",\"port\":3312,\"user\":\"talend_load\",\"passwd\":\"s9t5gNThn2vqWM7c\",\"db\":\"bigdata\",\"charset\":\"utf8\"}\n",
    "\n",
    "\n",
    "bigdata_conn = MysqlConn(json.dumps(bigdata_conn)).connect()\n",
    "# dateyesdbp_conn = MysqlConn(json.dumps(d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"select * from bigdata.report_htmls where publish_date > '2017-01-01' and publish_date < '2018-01-01'\"\n",
    "df = pd.read_sql(sql, bigdata_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(906219, 16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>report_id</th>\n",
       "      <th>title</th>\n",
       "      <th>tool</th>\n",
       "      <th>html_s3_url</th>\n",
       "      <th>convert_status</th>\n",
       "      <th>convert_task_id</th>\n",
       "      <th>file_size</th>\n",
       "      <th>tried_counts</th>\n",
       "      <th>report_type</th>\n",
       "      <th>absolute_pdf_s3_url</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>zs_auto_category</th>\n",
       "      <th>error_reason</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23389</td>\n",
       "      <td>14781164</td>\n",
       "      <td>關連交易 - 根據A股限制性股票激勵計劃向關連激勵對象授予限制性股票</td>\n",
       "      <td>solid_pdf_tool</td>\n",
       "      <td>/pipeline/data_report_html_so_ede830d864e23753...</td>\n",
       "      <td>done</td>\n",
       "      <td>1w1wdr5HscnRKtOgXmGd4Q</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-01-05 13:20:05</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2016-10-11 17:47:14</td>\n",
       "      <td>2016-10-12 09:56:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  report_id                               title            tool  \\\n",
       "0  23389   14781164  關連交易 - 根據A股限制性股票激勵計劃向關連激勵對象授予限制性股票  solid_pdf_tool   \n",
       "\n",
       "                                         html_s3_url convert_status  \\\n",
       "0  /pipeline/data_report_html_so_ede830d864e23753...           done   \n",
       "\n",
       "          convert_task_id  file_size  tried_counts  report_type  \\\n",
       "0  1w1wdr5HscnRKtOgXmGd4Q     1000.0             0          NaN   \n",
       "\n",
       "  absolute_pdf_s3_url        publish_date zs_auto_category error_reason  \\\n",
       "0                None 2017-01-05 13:20:05             None         None   \n",
       "\n",
       "           created_at          updated_at  \n",
       "0 2016-10-11 17:47:14 2016-10-12 09:56:19  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = df[~df.absolute_pdf_s3_url.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = subdf.sort_values('publish_date').drop_duplicates('zs_auto_category', keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 16)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf = subdf[['report_id','publish_date','absolute_pdf_s3_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report_id</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>absolute_pdf_s3_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67201</th>\n",
       "      <td>16141652</td>\n",
       "      <td>2017-05-06</td>\n",
       "      <td>http://cluster-s3nginx-inner.datayes.com:80/pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626474</th>\n",
       "      <td>16810886</td>\n",
       "      <td>2017-10-26</td>\n",
       "      <td>http://cluster-s3nginx-inner.datayes.com:80/pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645752</th>\n",
       "      <td>16862353</td>\n",
       "      <td>2017-11-07</td>\n",
       "      <td>http://cluster-s3nginx-inner.datayes.com:80/pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665319</th>\n",
       "      <td>16928344</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>http://cluster-s3nginx-inner.datayes.com:80/pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683777</th>\n",
       "      <td>18291473</td>\n",
       "      <td>2017-12-14</td>\n",
       "      <td>http://cluster-s3nginx-inner.datayes.com:80/pi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        report_id publish_date  \\\n",
       "67201    16141652   2017-05-06   \n",
       "626474   16810886   2017-10-26   \n",
       "645752   16862353   2017-11-07   \n",
       "665319   16928344   2017-11-28   \n",
       "683777   18291473   2017-12-14   \n",
       "\n",
       "                                      absolute_pdf_s3_url  \n",
       "67201   http://cluster-s3nginx-inner.datayes.com:80/pi...  \n",
       "626474  http://cluster-s3nginx-inner.datayes.com:80/pi...  \n",
       "645752  http://cluster-s3nginx-inner.datayes.com:80/pi...  \n",
       "665319  http://cluster-s3nginx-inner.datayes.com:80/pi...  \n",
       "683777  http://cluster-s3nginx-inner.datayes.com:80/pi...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf.to_csv(\"template.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(verify_msgs)\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.title(\"verification result\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.getCollection('report_html').update({\"taskId\" : \"c4OWMpFBIfyFdzk5Actm9Q\"},{$set:{\"progress\" : \"undo\"}})\n",
    "# db.getCollection('report_html').find({\"taskId\" : \"c4OWMpFBIfyFdzk5Actm9Q\"})\n",
    "# db.getCollection('report_html').find({\"taskId\" : \"-j5944Z0ahBqRuNHWHpA0w\"})\n",
    "\n",
    "# db.getCollection('report_html').({\"receiveTime\" : {$lt:\"2019-06-28\"},\"progress\" : \"undo\"})\n",
    "# \"receiveTime\" : \"2019-05-16\n",
    "\n",
    "# db.getCollection('report_html').updateMany({\"progress\" : \"converting\"},{$set:{\"progress\" : \"undo\"}})\n",
    "# db.getCollection('report_html').updateMany({\"progress\" : \"wait\"},{$set:{\"progress\" : \"undo\"}})\n",
    "\n",
    "# -- to \n",
    "# db.getCollection('report_html').find({\"receiveTime\":{$exists:0}, \"progress\" : \"undo\"}).count()\n",
    "# db.getCollection('report_html').remove({\"receiveTime\":{$exists:0}})\n",
    "\n",
    "# var isoDate = doc.t;   // t is correct key?     \n",
    "#    var isoString = isoDate.toISOString()    \n",
    "#    // update the collection with string using a new key    \n",
    "#    db.events.update({\"_id\":doc._id},{$set:{\"iso_str\":isoString} );\n",
    "#    isoDate.toJSON().substr(9, 20);\n",
    "# db.getCollection('report_html').find({\"receiveTime\":{$exists:0}}).forEach( function (x) {   \n",
    "#   var isoDate = x.publishDate\n",
    "#   var isoString = isoDate.toISOString()\n",
    "  \n",
    "#   x.publishDate = isoString.substr(0,10); // convert field to string\n",
    "#   db.report_html.save(x);\n",
    "# });\n",
    "\n",
    "\n",
    "# db.getCollection('report_html').updateMany({\"progress\" : \"undo\", \"publishDate\" : {$lt:ISODate(\"2019-07-09T00:00:00.000Z\")}},{$set:{\"progress\" : \"wait\"}})\n",
    "\n",
    "# db.getCollection('report_html').updateMany({\"progress\" : \"undo\", \"publishDate\" : {$lt:\"2019-07-01\"}},{$set:{\"progress\" : \"wait\"}})\n",
    "# db.getCollection('report_html').find({\"progress\" : \"undo\"}).count()\n",
    "\n",
    "# db.getCollection('report_html').find({\"taskId\" : \"\"}).count()\n",
    "\n",
    "# db.getCollection('report_html').find({\"progress\" : \"undo\", \"publishDate\" : {$lt:\"2019-07-01\"}}).count()\n",
    "#  publishdate=2018-11-13 00:00:00\n",
    "# db.getCollection('report_html').find({\"publishDate\" : \"2018-11-13 00:00:00\"})\n",
    "# db.getCollection('report_html').find({\"progress\" : \"undo\", \"publishDate\" : \"2019-07-02\"}).count()\n",
    "# db.getCollection('report_html').find({\"progress\" : \"undo\", \"publishDate\" : \"2019-07-03\"}).count()\n",
    "# db.getCollection('report_html').find({\"progress\" : \"undo\", \"publishDate\" : \"2019-07-04\"}).count()\n",
    "# db.getCollection('report_html').find({\"progress\" : \"undo\", \"publishDate\" : {$lt:\"2019-07-05\"}}).count()\n",
    "\n",
    "# db.getCollection('report_html').find({\"progress\" : \"undo\", \"receiveTime\":{$exists:1}}).count()\n",
    "# db.getCollection('report_html').find({\"progress\" : \"undo\"}).count()\n",
    "\n",
    "# db.getCollection('report_html').find({\"progress\": \"undo\"}).sort({\"publishDate\":-1})\n",
    "                    \n",
    "#                     sort=[('publishDate', DESCENDING)],\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################查看当前转换情况 \n",
    "# SELECT \n",
    "#     date_format(ifnull(updated_at, created_at), '%Y-%m-%d %H') '时间', \n",
    "#     sum(case when convert_status = 'pending' then 1 else 0 end) pending,\n",
    "#     sum(case when convert_status = 'transferred' then 1 else 0 end) transferred,\n",
    "#     sum(case when convert_status = 'uploaded' then 1 else 0 end) uploaded,\n",
    "#     sum(case when convert_status = 'done' then 1 else 0 end) done,\n",
    "#     sum(case when convert_status = 'failed' then 1 else 0 end) failed,\n",
    "#     sum(case when convert_status = 'timeout' then 1 else 0 end) timeout,\n",
    "#     avg(minute(timediff(updated_at, created_at)))\n",
    "# FROM\n",
    "#     bigdata.report_htmls\n",
    "# WHERE\n",
    "#     PUBLISH_DATE > DATE_SUB(NOW(), INTERVAL 10 DAY)\n",
    "#         AND created_at > DATE_SUB(NOW(), INTERVAL 10 DAY)\n",
    "# group by date_format(ifnull(updated_at, created_at), '%Y-%m-%d %H')\n",
    "# order by date_format(ifnull(updated_at, created_at), '%Y-%m-%d %H') desc\n",
    "\n",
    "\n",
    "\n",
    "# #################################################################\n",
    "#                  select m.news_id, m.news_title as title, m.update_time as update_time,\n",
    "#                  l.longsummary_withtag as summary, substr(m.news_title, 1,2) as s_type\n",
    "#                  from bigdata.news_longsummary_withtag l,\n",
    "#                     (select news_id, update_time, news_title from news.news_metadata\n",
    "#                      where (news_site_name like '%证券之星%' or (news_site_name='搜狐财经' and news_author='央视财经') or news_site_name='金融界')\n",
    "#                      and news_title like '午评%'\n",
    "#                      and update_time >= '2019-01-17 14:55' and update_time <= '2019-02-28 17:00'\n",
    "#                      and insert_time >= '2019-01-17'\n",
    "#                      union\n",
    "#                      select news_id, update_time, concat(replace(substring(news_title,1,2),'开盘','早评'),\n",
    "#                      substring(news_title,3))\n",
    "#                      from news.news_metadata\n",
    "#                      where insert_time >= '2019-01-17'\n",
    "#                      and news_site_name='金融界'\n",
    "#                      AND page_tag = '金融界首页>股票频道>市况直击'\n",
    "#                      AND news_title like '开盘%'\n",
    "#                      ) m\n",
    "#                  where l.news_id=m.news_id and substr(m.news_title, 1,2)='午评'\n",
    "# #################################################################\n",
    "# 412712\n",
    "\n",
    "# select * from news.news_metadata \n",
    "#                      where news_site_name like '%证券之星%' \n",
    "#                      and news_title like '午评%' \n",
    "# order by update_time desc limit 10\n",
    "\n",
    "# select * from news.news_metadata \n",
    "#                      where news_title like '午评%' and news_site_name like '%金融界%' \n",
    "                      \n",
    "# order by update_time desc limit 100\n",
    "# #################################################################\n",
    "# select * from news.news_metadata \n",
    "#                      where news_title like '午评%' \n",
    "# order by update_time desc limit 100\n",
    "# #################################################################\n",
    "# select count(*) from bigdata.report_htmls \n",
    "# where tool='solid_pdf_tool' and convert_status='failed' and created_at>='2019-04-25'\n",
    "\n",
    "# select * from rr_main\n",
    "\n",
    "# select * from bigdata.report_htmls\n",
    "# where convert_task_id='jtU_8yt4CUjSmHCXl9DLVg'\n",
    "\n",
    "\n",
    "# select * from bigdata.report_htmls\n",
    "# where updated_at>='2019-06-27' order by updated_at desc\n",
    "\n",
    "# select distinct(report_id) \n",
    "# select * from bigdata.report_htmls where html_s3_url like '%data_report_html_db77e58b34b7f2faab365afc031b663d%'\n",
    "\n",
    "# select * from bigdata.report_htmls \n",
    "# where report_id in (29825543)\n",
    "# where convert_status='undo'\n",
    "# select * from bigdata.report_htmls \n",
    "# where convert_task_id='gVRxFgMTCyoOyQC_lBC5aQ'\n",
    "\n",
    "# select * from bigdata.report_htmls order by updated_at desc\n",
    "# select * from bigdata.report_htmls order by publish_date desc\n",
    "# select * from bigdata.report_htmls where convert_status='undo' \n",
    "# order by publish_date desc\n",
    "# where updated_at>='2019-06-29' and  updated_at<'2019-07-02 14:25' and tool='datayes_api'\n",
    "\n",
    "# select * from bigdata.report_htmls where updated_at>='2019-06-29' and  updated_at<'2019-06-30' and tool='datayes_api'\n",
    "\n",
    "# \t4722937\t28991995\t杭州银行投资者关系活动记录表_20181109\tdatayes_api\t/pipeline/data_report_html_468f4e5d2123871f9b1af4c797c8304f.html\tuploaded\tAxZf83bY7mptyyaqTOsCyQ\t0\t1\t1\thttp://cluster-s3nginx-inner.datayes.com:80/pipeline/report/2018-11-13/20181113_12024c08d4ae00442fee58220c9a9b6f55be51a8f.pdf\t2018-11-13 00:00:00\t投资者关系-日常经营\t\t2018-11-13 14:20:10\t2019-06-29 11:56:34\n",
    "# 上面这记录重置为undo, 会写进mongo中\n",
    "\n",
    "#  and tool='solid_pdf_tool'\n",
    "# select * from bigdata.report_htmls where updated_at like '2019-06-28%'\n",
    "# select * from bigdata.report_htmls where updated_at>='2019-07-01'\n",
    "# select count(*) from bigdata.report_htmls where updated_at>='2019-06-29' and tool='datayes_api'\n",
    "# convert_status='pending' and tool !='doc'\n",
    "\n",
    "#  order by updated_at desc\n",
    "# select * from bigdata.rb_report_htmls where convert_status=1 subtitle is not null order by updated_at desc\n",
    "# select * from bigdata.rb_report_htmls order by updated_at desc\n",
    "# delete from bigdata.rb_report_htmls where error_reason='empty html'\n",
    "\n",
    "# select id, report_id, title, convert_task_id, absolute_pdf_s3_url, publish_date, zs_auto_category, created_at\n",
    "#                 from bigdata.report_htmls\n",
    "#                 where report_id not in (select distinct(report_id) from bigdata.rb_report_htmls)\n",
    "#                 and zs_auto_category like '%招股书%' and absolute_pdf_s3_url is not null and convert_task_id is not null \n",
    "\n",
    "# select distinct(report_id) from bigdata.rb_report_htmls\n",
    "\n",
    "# select * from bigdata.report_htmls \n",
    "# where convert_task_id in ('fWMfIKhYNt4hOcDxndwLJQ','sXSpj3_TzFfNOSkUV2OWDQ','r4X8nVerGZc-vwqjiITy0A','y38zaGw5RKZqU7ljT5Q2JA')\n",
    "\n",
    "# select * from bigdata.report_htmls \n",
    "# where report_id in (29692422)\n",
    "\n",
    "\n",
    "# 29660806,\n",
    "# 29660810,\n",
    "# 29660809,\n",
    "# 29660811,\n",
    "# 29660812,\n",
    "# 29660813,\n",
    "# 29660833,\n",
    "# 29660840,\n",
    "# 29611279,\n",
    "# 29650681)\n",
    "\n",
    "# where report_id=23442544\n",
    "# db_rb_report_htmls\n",
    "# where tool='solid_pdf_tool' and convert_status='done' and created_at>='2019-04-25'\n",
    "\n",
    "# where updated_at>='2019-04-26 11' and updated_at<='2019-04-26 12'  # 561 \n",
    "\n",
    "\n",
    "\n",
    "# select count(*) from bigdata.report_htmls \n",
    "# where updated_at>='2019-04-26 12' and updated_at<='2019-04-26 13'   #343\n",
    "\n",
    "# select count(*) from bigdata.report_htmls \n",
    "# where updated_at>='2019-04-26 13' and updated_at<='2019-04-26 14'   #733 \n",
    "\n",
    "# select count(*) from bigdata.report_htmls \n",
    "# where updated_at>='2019-04-26 14' and updated_at<='2019-04-26 15'   #733 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# select * from bigdata.report_htmls \n",
    "# where updated_at>='2019-04-26' and tool='datayes_api' and convert_status='done' and tool='solid_pdf_tool'\n",
    "# #################################################################\n",
    "# select * from bigdata.report_htmls \n",
    "# where convert_status='pending'\n",
    "# where report_id=29546630\n",
    "# where convert_status='pending'\n",
    "\n",
    "# ###############################################################\n",
    "# # 从report_htmls中筛选数据， 找出report_id, 根据这批report_id到announcement 表中修改相应列为report_htmls中的值。\n",
    "\n",
    "# 陈翻(fan.chen) 3-21 10:29:31\n",
    "# update bigdata.announcement a inner join bigdata.report_htmls rh on a.id = rh.report_id set a.html_s3_url = rh.html_s3_url \n",
    "# where rh.convert_status = 'uploaded' and rh.tool = 'datayes_api' and rh.html_s3_url is not null\n",
    "\n",
    "\n",
    "# select count(*) from bigdata.report_htmls \n",
    "# where convert_status='pending'\n",
    "\n",
    "\n",
    "# select convert_task_id, convert_status from bigdata.report_htmls where convert_status='undo'\n",
    "\n",
    "# select count(*) from bigdata.rb_report_htmls \n",
    "# select count(*) from bigdata.rb_report_htmls \n",
    "# select * from bigdata.report_htmls \n",
    "# where created_at >= '2019-01-18' and \n",
    "\n",
    "# #########################################################################################3\n",
    "# # 查询两个tool的转换结果都是fail的记录。 \n",
    "# select * from bigdata.report_htmls \n",
    "# where tool ='solid_pdf_tool' and convert_status='failed' and error_reason like '%valid%' \n",
    "# and report_id in\n",
    "# \t(\n",
    "# \tselect report_id from bigdata.report_htmls \n",
    "# \twhere convert_status='failed' and tool='datayes_api' and created_at >= '2019-04-24'\n",
    "# \t)\n",
    "# #############################################################################################\n",
    "\n",
    "# where convert_status='pending'\n",
    "# where created_at >= '2019-01-16' and convert_status='timeout'\n",
    "\n",
    "# where absolute_pdf_s3_url like '%http://cluster-s3nginx-inner.datayes.com/pipeline/datayes/pipeline/data/report/sh/SH601811CN/20160719_601811.SH__新华文轩首次公开发行A股股%'\n",
    "# select * from bigdata.report_htmls  where report_id=29564558\n",
    "# select * from bigdata.rb_report_htmls order by updated_at desc limit 10\n",
    "\n",
    "# select * from bigdata.report_htmls where convert_status='done' and zs_auto_category like '%年报%' order by created_at desc limit 100 \n",
    "\n",
    "# select * from bigdata.report_htmls\n",
    "# where report_id in (29669006\n",
    "# ,29349434\n",
    "# ,29528207,29559365)\n",
    "\n",
    "# select count(*) from bigdata.announcement a\n",
    "# where a.zs_auto_category = '资产买卖-日常经营' and a.publish_date >= '2019-01-01'\n",
    "\n",
    "# select * from bigdata.rb_report_htmls order by updated_at desc\n",
    "# select * from bigdata.report_htmls\n",
    "# where convert_task_id='A4o-kYKpc5Jz-UD9BFcirw'\n",
    "# where report_id in (29660902,29528207,29559365)\n",
    "\n",
    "# db_rb_report_htmls={\"host\":\"security03-dev.datayes.com\",\"port\":3306,\"user\":\"talend_load\",\"passwd\":\"NCph1G9BQT3DuQj\",\"db\":\"bigdata\",\"charset\":\"utf8\"}\n",
    "\n",
    "\n",
    "# select * from bigdata.announcement\n",
    "\n",
    "# where id in (570988)\n",
    "# where convert_task_id='IJJ7fJSzP9SgETybL7rZiw'\n",
    "# where report_id in (29190921), 29010155,28972154,28971240,\n",
    "# 28970802) 28972259\n",
    "\n",
    "# where convert_task_id='qCdYi7YOhYOFS8bzelCROA'\n",
    "# where tool='datayes_api' and convert_status='done'\n",
    "\n",
    "# where tool='datayes_api' and updated_at like '%2018-10-26%'\n",
    "# where tool='datayes_api' and updated_at like '%2018-10-27%' and convert_status='done'\n",
    "\n",
    "\n",
    "# select * from news.report_meta\n",
    "\n",
    "# where  title like '%招股说明书%'\n",
    "\n",
    "# where tool='datayes_api' and convert_status='failed'\n",
    "# where id = 1004449\n",
    "\n",
    "# where tool='datayes_api' and convert_status=''\n",
    "\n",
    "# 早评：\n",
    "# SELECT * FROM news.news_metadata WHERE news_site_name='金融界' AND page_tag = '金融界首页>股票频道>市况直击' AND news_title like '开盘%'\n",
    "# 收评：\n",
    "# SELECT * FROM news.news_metadata WHERE insert_time >= '2018-10-01'  AND news_site_name='搜狐财经' AND news_author = '央视财经' AND news_title like '收评%'\n",
    "\n",
    "# select m.news_id, m.news_title as title, m.update_time as time,  l.longsummary_withtag as summary, substr(m.news_title, 1,2) as s_type\n",
    "# from bigdata.news_longsummary_withtag l,\n",
    "# (select news_id, update_time, news_title from news.news_metadata where insert_time >= '2018-10-01' AND news_site_name='搜狐财经' AND news_author = '央视财经' AND news_title like '收评%') m\n",
    "# where l.news_id=m.news_id\n",
    "# order by m.update_time desc limit 10\n",
    "\n",
    "# select m.news_id, m.news_title as title, m.update_time as time,  l.longsummary_withtag as summary, substr(m.news_title, 1,2) as s_type\n",
    "# from bigdata.news_longsummary_withtag l,\n",
    "# (select news_id, update_time, news_title from news.news_metadata where insert_time >= '2018-11-01' and news_site_name='金融界' AND page_tag = '金融界首页>股票频道>市况直击' AND news_title like '开盘%') m\n",
    "# where l.news_id=m.news_id\n",
    "# order by m.update_time desc limit 10\n",
    "\n",
    "# select news_id, update_time, news_title from news.news_metadata \n",
    "# where insert_time >= '2018-11-01' and news_site_name like '%证券之星%' and substr(news_title, 1,2) in (\"收评\",\"午评\",\"早评\") \n",
    "\n",
    "# select news_id, update_time, news_title from news.news_metadata\n",
    "#                      where (news_site_name like '%证券之星%' or (news_site_name='搜狐财经' and news_author='央视财经') or news_site_name='金融界'\n",
    "# \t\t\t\t\t\t\tor news_site_name = '财联社')\n",
    "#                      and news_title like '午评%'\n",
    "#                      and update_time >= '2019-01-17 ' \n",
    "#                      and insert_time >= '2019-01-17'\n",
    "\n",
    "# select * from news.news_metadata\n",
    "# where news_site_name = '财联社' order by insert_time desc\n",
    "\n",
    "\n",
    "# select * from news.news_metadata\n",
    "# where\n",
    "# source_name = '财联社' and news_title like '%评%' order by insert_time desc\n",
    "\n",
    "# SELECT * FROM news_hotspots WHERE SOURCE_NAMES = '财联社' AND NEWS_SECTION = '电报' AND CREATE_TIME > '2019-01-01' AND (TITLE LIKE '午评%' OR TITLE LIKE '收评%')\n",
    "\n",
    "# select m.news_id, m.news_title as title, m.update_time as time,  l.longsummary_withtag as summary, substr(m.news_title, 1,2) as s_type\n",
    "# from bigdata.news_longsummary_withtag l,\n",
    "# (\n",
    "# \tselect news_id, update_time, news_title from news.news_metadata where insert_time >= '2018-07-18' and (news_site_name like '%证券之星%' or news_site_name like '%金融界%') and substr(news_title, 1,2) in (\"收评\",\"午评\",\"早评\") \n",
    "# \tunion\n",
    "# \tselect news_id, update_time, concat(replace(substring(news_title,1,2),'开盘','早评'),substring(news_title,3)) \n",
    "# \tfrom news.news_metadata \n",
    "# \twhere insert_time >= '2018-11-01' \n",
    "# \tand news_site_name='金融界' \n",
    "# \tAND page_tag = '金融界首页>股票频道>市况直击' \n",
    "# \tAND news_title like '开盘%'\n",
    "# ) m\n",
    "# where l.news_id=m.news_id\n",
    "# order by m.update_time desc limit 10\n",
    "\n",
    "\n",
    "\n",
    "# select *\n",
    "#                      from news.news_metadata \n",
    "#                      where insert_time >= '2018-11-06' \n",
    "#                      and news_site_name='金融界' \n",
    "#                      AND page_tag = '金融界首页>股票频道>市况直击' \n",
    "# order by update_time desc limit 10\n",
    "\n",
    "#                  select m.news_id, m.news_title as title, m.update_time as update_time, l.longsummary_withtag as summary, substr(m.news_title, 1,2) as s_type\n",
    "#                  from bigdata.news_longsummary_withtag l, \n",
    "#                     (select news_id, update_time, news_title from news.news_metadata \n",
    "#                      where news_site_name like '%证券之星%' \n",
    "#                      and news_title like '午评%' \n",
    "#                      and update_time >= '2018-11-05 11:20' and update_time <= '2018-11-05 13:00' \n",
    "#                      and insert_time >= '2018-11-05'\n",
    "#                      union\n",
    "#                      select news_id, update_time, concat(replace(substring(news_title,1,2),'开盘','早评'),substring(news_title,3)) \n",
    "#                      from news.news_metadata \n",
    "#                      where insert_time >= '2018-11-05' \n",
    "#                      and news_site_name='金融界' \n",
    "#                      AND page_tag = '金融界首页>股票频道>市况直击' \n",
    "#                      AND news_title like '开盘%'\n",
    "#                      ) m\n",
    "\n",
    "#                  where l.news_id=m.news_id and substr(m.news_title, 1,2)='午评'\n",
    "\n",
    "# select news_id, update_time, news_title from news.news_metadata \n",
    "#                      where news_site_name like '%证券之星%' \n",
    "#                      and news_title like '午评%' \n",
    "#                      and update_time >= '2018-11-05 11:20'\n",
    "# order by update_time desc\n",
    "# select * from news.news_metadata where insert_time >= '2018-07-18'  and substr(news_title, 1,2) in (\"收评\",\"午评\",\"早评\")\n",
    "\n",
    "# select m.news_id, m.news_title as title, m.update_time as time,  l.longsummary_withtag as summary, substr(m.news_title, 1,2) as s_type\n",
    "# from bigdata.news_longsummary_withtag l,\n",
    "# (select news_id, update_time, news_title from news.news_metadata where insert_time >= '2018-11-01'  and substr(news_title, 1,2) in (\"收评\",\"午评\",\"早评\")) m\n",
    "# where l.news_id=m.news_id\n",
    "# order by m.update_time desc limit 100\n",
    "\n",
    "# select * from bigdata.report_htmls \n",
    "# where tool='solid_pdf_tool' and convert_status='pending' and created_at>='2018-10-26'\n",
    "# where tool='solid_pdf_tool' and convert_status='pending' and convert_task_id is not NULL\n",
    "# where tool='solid_pdf_tool' and convert_status='pending' and created_at>='2018-10-26'\n",
    "\n",
    "# select id AS outKey, report_id, insert_time, stockid, url, zs_auto_category AS category  from bigdata.announcement \n",
    "# where stockid is not null and report_type=1 and stockid=600193\n",
    "# order by insert_time desc limit 10\n",
    "\n",
    "# select * from bigdata.report_htmls \n",
    "# where convert_status = 'pending'\n",
    "\n",
    "\n",
    "# where report_id=15640419\n",
    "# select * from bigdata.announcement\n",
    "# 16472150\n",
    "# 16472150\n",
    "# where zs_auto_category='招股书-IPO' and updated_at<'2018'\n",
    "# where convert_status='pending' and zs_auto_category='招股书-IPO'\n",
    "# use bigdata;\n",
    "\n",
    "\n",
    "# SELECT \n",
    "#     sum(case when convert_status = 'pending' then 1 else 0 end) pending,\n",
    "#     sum(case when convert_status = 'transferred' then 1 else 0 end) transferred,\n",
    "#     sum(case when convert_status = 'uploaded' then 1 else 0 end) uploaded,\n",
    "#     sum(case when convert_status = 'done' then 1 else 0 end) done,\n",
    "#     sum(case when convert_status = 'failed' then 1 else 0 end) failed,\n",
    "#     sum(case when convert_status = 'timeout' then 1 else 0 end) timeout\n",
    "# FROM \n",
    "#     bigdata.report_htmls\n",
    "# WHERE\n",
    "# \ttool='solid_pdf_tool' and zs_auto_category like '%招股书%'  and updated_at > DATE_SUB(NOW(), INTERVAL 10 DAY)\n",
    "\n",
    "# select a.report_id, a.title, a.convert_status from report_htmls a \n",
    "# select * from bigdata.report_htmls \n",
    "# where created_at>='2019-03-01' and convert_status='failed' and error_reason like '%s3%' '%error while read result file%'\n",
    "# select * from report_htmls\n",
    "# where convert_status='failed' and error_reason like '%error while read result file%'\n",
    "# select * from bigdata.report_htmls\n",
    "# where report_id in (28982074) , 16913780,28871773, 28875501)\n",
    "\n",
    "# select * from bigdata.announcement order by insert_time desc limit 10\n",
    "# where id='4703896'\n",
    "# where report_id in (\n",
    "# \tSELECT ID FROM bigdata.announcement\n",
    "# \tWHERE report_id LIKE '%股东%大会%通知%'\n",
    "# \tAND scanned_time>='2018-10-29 15:00:00'\n",
    "# \tand id not in (select anno_id from bigdata.auto_equ_sh_meeting_notice) and convert_status='failed'\n",
    "# );\n",
    "\n",
    "# select is_open from md_trade_cal where exchange_cd='XSHG' and CALENDAR_DATE='%s' \n",
    "# select min(updated_at), max(updated_at) from bigdata.report_htmls \n",
    "\n",
    "# select * from bigdata.report_htmls \n",
    "# where title like '%至纯科技%' and updated_at > '2018-11-01'\n",
    "\n",
    "# where  convert_status='pending'\n",
    "# 2018-10-30 23:08:59\t2018-10-31 04:47:21\n",
    "# where title like '%深圳市麦达%'\n",
    "# select * from bigdata.report_htmls \n",
    "# where zs_auto_category like '%招股书%' and convert_status='done' and updated_at > '2018-10-30' order by updated_at desc\n",
    "# and error_reason like '%except%'\n",
    "# where id in (4692430,4690432,4690436,4692405,4692420,4692421,4692423,4692428,4692430)\n",
    "# where zs_auto_category like '%招股书%' and error_reason like '%except%'\n",
    "# select * from bigdata.report_htmls \n",
    "# where id=4692908 180上重转看能否转换成功\n",
    "# select * from bigdata.report_htmls \n",
    "# where report_id in (994764, 996272, 1020088, 1028737, 994764 )\n",
    "# where report_id in (1031782, 996272,994764, 1025266)\n",
    "\n",
    "\n",
    "# select * from bigdata.report_htmls \n",
    "# where convert_status=\"pending\"\n",
    "# where  tool=\"datayes_api\" and updated_at >= '2018-11-11'\n",
    "\n",
    "# where convert_status=\"done\" and tool=\"datayes_api\"\n",
    "# where convert_status=\"pending\"\n",
    "# where updated_at >= '2018-12-10' \n",
    "# and tool = \"solid_pdf_tool\" and convert_status=\"done\"\n",
    "# and updated_at <= '2018-11-12 02' and convert_status='failed'\n",
    "\n",
    "# select * from bigdata.report_htmls \n",
    "# where report_id=570988 report_html表里找不到记录\n",
    "# where updated_at >= '2018-11-11'\n",
    "# where convert_status='pending' and convert_task_id is not null\n",
    "# select * from bigdata.report_htmls \n",
    "# where convert_task_id='HB6jmJMFi04yJ31fYcjgkw'\n",
    "# where zs_auto_category like '%招股书%' and convert_status='failed' and tool=\"solid_pdf_tool\" and error_reason not like '%pdf%'\n",
    "# where zs_auto_category like '%招股书%' and convert_status='failed' and tool=\"datayes_api\" and error_reason not like '%pdf%'\n",
    "# error_reason like '%except%'\n",
    "# where tool='solid_pdf_tool' and zs_auto_category like '%招股书%' and updated_at > DATE_SUB(NOW(), INTERVAL 10 DAY) and convert_status='pending'\n",
    "# select * from bigdata.report_htmls \n",
    "# where report_id in (994764, 996272, 1020088, 1028737, 994764 )\n",
    "# where id=4692445\n",
    "# where tool='solid_pdf_tool' and zs_auto_category like '%招股书%' and updated_at > DATE_SUB(NOW(), INTERVAL 10 DAY) and convert_status='failed'\n",
    "# where created_at>='2018-10-26' and error_reason like '%s3%'\n",
    "# select count(*) from bigdata.report_htmls \n",
    "# where convert_status='pending' and convert_task_id is NULL and error_reason like '%s3%' and created_at>='2018-10-29'\n",
    "# where convert_status='pending'\n",
    "# where tool='solid_pdf_tool' and convert_status='failed' and created_at>='2018-10-25'\n",
    "# http://cluster-s3nginx-inner.datayes.com:80/pipeline/report/2018-10-26/20181026_SZ002638_63302159.pdf\n",
    "# where error_reason not like '%empty%'\n",
    "# select * from bigdata.rb_report_htmls where convert_task_id=\"KCj-7mY53QCJ0D78XD5BhQ\"\n",
    "# delete from bigdata.rb_report_htmls where error_reason like '%empty%';\n",
    "# select count(*) from bigdata.rb_report_htmls \n",
    "# where error_reason like '%empty%'\n",
    "\n",
    "# select count(*) from bigdata.report_htmls \n",
    "\n",
    "# select tool, count(tool) from bigdata.report_htmls \n",
    "# where  created_at >= '2018-11-12' and convert_status='failed' \n",
    "# group by tool\n",
    "\n",
    "# select * from bigdata.report_htmls \n",
    "# where  created_at >= '2018-11-12' and convert_status='failed'  and tool='solid_pdf_tool' and error_reason is null\n",
    "# group by tool\n",
    "\n",
    "# and error_reason like '%none%' 474\n",
    "# where created_at >= '2018-11-12'  and convert_status='failed' and error_reason like '%none%'\n",
    "\n",
    "# select * from bigdata.report_htmls \n",
    "# where convert_status='pending' and zs_auto_category like '%招股书%'  convert_task_id is not NULL\n",
    "# select * from bigdata.report_htmls \n",
    "# where report_id=28991462\n",
    "# select * from bigdata.report_htmls \n",
    "# where error_reason like '%s3%' and created_at like '2018-10-29%' and convert_status='failed'\n",
    "# where convert_status='pending'\n",
    "\n",
    "# select * from bigdata.report_htmls \n",
    "# where tool='solid_pdf_tool' and title like '%修订稿%' order by updated_at desc\n",
    "# where tool='solid_pdf_tool' and zs_auto_category like '%增发%' order by updated_at desc\n",
    "\n",
    "# where tool='solid_pdf_tool' and zs_auto_category like '%招股书%' order by updated_at desc\n",
    "# select convert_status,created_at,updated_at,error_reason from bigdata.report_htmls \n",
    "# where tool='solid_pdf_tool' and zs_auto_category like '%招股书%' and error_reason like '%tool%' and convert_status='failed' order by updated_at desc\n",
    "\n",
    "# select distinct(convert_status) from bigdata.report_htmls \n",
    "# where tool='datayes_api' and convert_status='todo'\n",
    "# and convert_status='done'\n",
    "# select * from bigdata.report_htmls \n",
    "# select * from bigdata.report_htmls \n",
    "# select * from bigdata.report_htmls \n",
    "# where tool='datayes_api' and convert_status='failed' and error_reason is NULL like '%exception%'\n",
    "\n",
    "# select * from bigdata.report_htmls \n",
    "# where report_id in (29004822,29004820,28989839)\n",
    "# select * from bigdata.report_htmls \n",
    "# where updated_at >= '2018-11-16' and convert_status='done'\n",
    "# where tool='datayes_api' and convert_status='failed'  and error_reason is NULL\n",
    "# where zs_auto_category like '%招股书%' and  updated_at >'2018-10-20' and convert_status='failed' and error_reason is NULL\n",
    "# where zs_auto_category like '%招股书%' and  updated_at >'2018-10-20' and error_reason like '%exception%' and convert_status='failed'\n",
    "# where tool=''\n",
    "# where report_id='74138'\n",
    "# where convert_task_id='h2_XgNjDh8mtjaeiGY3X2w'\n",
    "\n",
    "# where convert_task_id='h2_XgNjDh8mtjaeiGY3X2w'\n",
    "# where zs_auto_category like '%IPO%' and updated_at >'2018-10-26' and convert_status='timeout'\n",
    "# where convert_status='pending' and created_at>'2018-10-26' and convert_task_id is not NULL\n",
    "# where convert_status='pending'\n",
    "# where convert_status='done' and created_at>='2018-10-28'\n",
    "# where convert_status='pending' and created_at>'2018-10-26' and convert_task_id is not NULL\n",
    "# where convert_status='done' and created_at>'2018-10-28'\n",
    "# where convert_status='pending' and created_at>'2018-10-26'\n",
    "\n",
    "# where convert_status='pending' and created_at>'2018-10-26' and convert_task_id is not NULL\n",
    "# where tool='datayes_api' and convert_status='failed' \n",
    "# where  tool='solid_pdf_tool' and convert_status='failed' and updated_at>'2018-10-26' and error_reason like '%timeout%'\n",
    "# and error_reason='tool convert fail: translate fail: InternalError'\n",
    "# where  tool='datayes_api' and convert_status='failed' and error_reason is NULL\n",
    "# where convert_task_id='MtTiPy6jgzAZ6YDDojSixA'\n",
    "# where convert_status='pending'\n",
    "# where  tool='datayes_api' and convert_status='failed'\n",
    "# where tool='datayes_api' and convert_status='failed' and error_reason not like '%pdf%'\n",
    "# where id=1041473\n",
    "# where convert_status='pending'\n",
    "# select * from bigdata.report_htmls \n",
    "# where id = 4692497\n",
    "# select * from bigdata.report_htmls \n",
    "# where tool='datayes_api' and convert_status='failed' and error_reason not like '%pdf%'\n",
    "# where tool='datayes_api' and convert_status in ('done' , 'transferred')\n",
    "# where tool='datayes_api' and convert_status='failed' and error_reason not like '%pdf%'\n",
    "\n",
    "# where tool='datayes_api' and convert_status='failed' and error_reason not like '%pdf%'\n",
    "# where tool='datayes_api' and convert_status in ('done' , 'transferred')\n",
    "# where tool='datayes_api' and convert_status='failed'\n",
    "\n",
    "# where tool='datayes_api' and convert_status='failed' and error_reason is null\n",
    "# where tool='datayes_api' and convert_status in ('done' , 'transferred')\n",
    "# where report_id = 15648094\n",
    "# select distinct(convert_status) from bigdata.report_htmls \n",
    "# where tool='datayes_api' and convert_status in ('done' , 'transferred')\n",
    "# where tool='datayes_api' and convert_status='failed' and error_reason like '%exception%'\n",
    "# where id=4690428\n",
    "\n",
    "# where created_at>='2018-10-26' and convert_status='failed' and error_reason is NULL\n",
    "# where created_at>='2018-10-25' and convert_status='failed' and error_reason like '%s3%'\n",
    "# where id=4690428\n",
    "# where report_id in (27848978,27848955)\n",
    "# where created_at>='2018-10-25' and convert_status='failed' and error_reason like '%s3%'\n",
    "# where tool='datayes_api'\n",
    "# where id=505459\n",
    "\n",
    "# where created_at>='2018-10-25' and convert_status='failed'\n",
    "\n",
    "# select * from bigdata.report_htmls \n",
    "# where convert_status='pending'\n",
    "# where  updated_at > '2018-08-03%' and zs_auto_category like '%招股书-IPO%'\n",
    "# \t4619591\t26441829\t首次公开发行股票招股说明书摘要\tsolid_pdf_tool\t\n",
    "# /pipeline/data_report_html_so_548c194943a851e0d078085f7c502ae9.html\tdone\tfMYqG0_ZUZLw7mIgan6FbA\t0\t9\t9\t\n",
    "# http://cluster-s3nginx-inner.datayes.com:80/pipeline/report/2018-08-22/20180822_SZ002935_11689777.pdf\t2018-08-22 00:00:00\t\n",
    "# 招股书-IPO\t\t2018-08-22 00:00:24\t2018-08-22 00:07:15\n",
    "# update bigdata.report_htmls set convert_status = 'pending', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null \n",
    "# where report_id\n",
    "# where id=4690428\n",
    "\n",
    "\n",
    "# select * from bigdata.report_htmls where report_id=21186597\n",
    "# where  updated_at > '2018-08-03%' and zs_auto_category like '%招股书-IPO%'\n",
    "#  convert_status=\"failed\" and\n",
    "\n",
    "# select m.news_id, m.news_title as title, m.update_time as time,  l.longsummary_withtag as summary, substr(m.news_title, 1,2) as s_type\n",
    "# from bigdata.news_longsummary_withtag l,\n",
    "# (select news_id, update_time, news_title from news.news_metadata where insert_time >= '2018-07-18' and news_site_name like '%证券之星%' and substr(news_title, 1,2) in (\"收评\",\"午评\",\"早评\")) m\n",
    "# where l.news_id=m.news_id\n",
    "# order by m.update_time desc limit 10\n",
    "\n",
    "# select distinct(zs_auto_category) from bigdata.report_htmls \n",
    "\n",
    "# and updated_at < '2018-07-22'\n",
    "# select * from bigdata.report_htmls where   updated_at > '2018-07-16 14' and error_reason = '未知'\n",
    "# update bigdata.report_htmls set convert_status = 'pending', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null \n",
    "\n",
    "# where updated_at > '2018-07-16 14' and error_reason = '未知'\n",
    "# convert_status=\"failed\" and updated_at > '2018-07-16 21' and updated_at < '2018-07-22'\n",
    "\n",
    "\n",
    "\n",
    "# use bigdata;\n",
    "# select * from report_htmls where report_type = 6 and convert_status=\"failed\" and error_reason like '%special code%'\n",
    "# ;\n",
    "# select error_reason , count(*) from report_htmls where report_type = 6 and convert_status=\"failed\" group by error_reason\n",
    "\n",
    "# select error_reason , count(*) from report_htmls where convert_status=\"failed\" group by error_reason\n",
    "\n",
    "# select convert_status , count(*) from bigdata.report_htmls  group by convert_status\n",
    "# pendig = 4448\n",
    "\n",
    "\n",
    "# select id AS outKey, report_id, insert_time, stockid, url, zs_auto_category AS category  \n",
    "# from bigdata.announcement \n",
    "# where stockid is not null and report_type=1 and id>=21034563\n",
    "\n",
    "# select id AS outKey, report_id, insert_time, stockid, url, zs_auto_category AS category  \n",
    "# from bigdata.announcement \n",
    "# where stockid is not null and report_type=1 order by id desc\n",
    "# 2103456321034563\n",
    "# 21034563\n",
    "\n",
    "# select a.id AS outKey, a.zs_auto_category AS category, a.stockid AS stockid, a.insert_time AS insert_time, a.auto_updated_time AS update_time, a.publish_date AS publish_date, a.report_id AS report_id, a.auto_report_category AS abstract, p.profitability AS profitability\n",
    "#             from bigdata.announcement a, bigdata.announcement_profitability p\n",
    "#             where a.stockid is not null and a.report_type=1 and a.id = p.report_id and a.id > 20400095\n",
    "\n",
    "# select convert_status , count(*) from bigdata.report_htmls where zs_auto_category like '%申报稿-IPO%' group by convert_status\n",
    "\n",
    "# select report_id, title, convert_task_id, absolute_pdf_s3_url, publish_date, zs_auto_category from bigdata.report_htmls \n",
    "# where zs_auto_category like '%招股书-IPO%'  and absolute_pdf_s3_url is not null and id >\n",
    "# and convert_status=\"failed\" order by id desc\n",
    "\n",
    "# s9t5gNThn2vqWM7c\n",
    "# select * from report_htmls where report_type = 6 and convert_status=\"failed\" and error_reason like 'tool convert done, but fail to postConvert'\n",
    "# 18273306\n",
    "# select * from bigdata.report_htmls where report_id in (21128735)\n",
    "# update bigdata.report_htmls set convert_status = 'pending', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null \n",
    "# where report_id in (21128735);\n",
    "\n",
    "# select * from bigdata.report_htmls where report_id in (24824966)\n",
    "\n",
    "\n",
    "# select * from bigdata.report_htmls where zs_auto_category like '%招股%' and convert_status = \"failed\"\n",
    "# 20266260\n",
    "# -- update bigdata.report_htmls set convert_status = 'pending', convert_task_id = null, updated_at = null, html_s3_url = null where report_id = 18880489\n",
    "# and error_reason in (\"\", \"tool convert done, but fail to postConvert\")\n",
    "\n",
    "# select * from bigdata.report_htmls where error_reason like \"%tool convert done, but fail to postConvert%\" \n",
    "\n",
    "# select * from bigdata.report_htmls where report_type = 6 and convert_status=\"failed\" and error_reason like \"%fail to postConvert%\"\n",
    "# --update bigdata.report_htmls set convert_status = 'pending', convert_task_id = null, updated_at = null, html_s3_url = null where report_type = 6 and convert_status=\"failed\" and error_reason like \"%fail to postConvert%\"\n",
    "# --update bigdata.report_htmls set convert_status = 'pending', convert_task_id = null, updated_at = null, html_s3_url = null where report_type = 6 and convert_status=\"failed\" and error_reason is NULL \n",
    "# --update bigdata.report_htmls set convert_status = 'pending', convert_task_id = null, updated_at = null, html_s3_url = null where report_id in (18866303);\n",
    "\n",
    "# select * from report_htmls where convert_status != \"done\" and \n",
    "# report_id in (19164911 ,19164791 ,19163791,19162793,19164216,19162793,18891818,18888358,18892470,18891167,19161632,19169186,19169094,19169186,\n",
    "# 15303730,15303730,15303477,15303730,15303576,15303710,15304315,15304838,14771464)\n",
    "\n",
    "# 2018-01-18 09:41:58,668 INFO:convert report 14771464 with url is None\n",
    "# 2018-01-18 09:42:00,686 INFO:convert report 15303477 with url is None\n",
    "# 2018-01-18 09:42:02,700 INFO:convert report 15303576 with url is None\n",
    "# 2018-01-18 09:42:04,726 INFO:convert report 15303710 with url is None\n",
    "# 2018-01-18 09:42:04,726 INFO:convert report 15303730 with url is None\n",
    "# 2018-01-18 09:42:04,743 INFO:convert report 15304315 with url is None\n",
    "# 2018-01-18 09:42:04,743 INFO:convert report 15304838 with url is None\n",
    "\n",
    "\n",
    "# select * from bigdata.report_htmls where\n",
    "# report_id in (19433817,19391176)\n",
    "\n",
    "\n",
    "# select * from report_htmls where\n",
    "# report_id in (14771464,15303477,15303576,15303710,15303730,15304315,15304838)\n",
    "\n",
    "# 16908010  16903972  16908014  16903966  16907966 16935432\n",
    "\n",
    "# select * from bigdata.report_htmls where convert_status=\"timeout\" and created_at >= \"2018-03-07\";\n",
    "# select * from bigdata.report_htmls where\n",
    "# report_id in (19346475,\n",
    "# 19346656,\n",
    "# 19346881,\n",
    "# 19346977,\n",
    "# 19347247,\n",
    "# 19347201)\n",
    "# update bigdata.report_htmls set convert_status = 'pending', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null where report_id in (16908010,16903972,16908014,16903966,16907966,16935432);\n",
    "# update bigdata.report_htmls set convert_status = 'pending', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null where report_id in (16908010,16908014);\n",
    "\n",
    "# # by given report_id\n",
    "# select distinct(convert_status) from bigdata.report_htmls where created_at >= \"2018-07-16 14\"\n",
    "# select * from bigdata.report_htmls where created_at >= \"2018-07-16 14\" and convert_status=\"fail\" order by created_at \n",
    "# select * from bigdata.report_htmls where created_at >= \"2018-07-16 14\" and convert_status=\"pending\" order by created_at \n",
    "# select * from bigdata.report_htmls where created_at >= \"2018-07-16 14\" and convert_status=\"timeout\" order by created_at \n",
    "# select * from bigdata.report_htmls where created_at >= \"2018-07-10\" and convert_status=\"timeout\" order by created_at \n",
    "# select * from bigdata.report_htmls where created_at >= \"2018-07-16 14\" order by created_at \n",
    "#  and zs_auto_category like '%年报%';\n",
    "# select * from bigdata.report_htmls \n",
    "# where convert_task_id in (\"rtXqWkvGX3V0N1Up2f4D9w\" , \"iUW7VU67zZLIqd6K5CPaJw\", \"SohH9390RWt0ZOCiu0tOWQ\")\n",
    "# update bigdata.report_htmls set convert_status = 'pending', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null \n",
    "# where convert_task_id=\"iUW7VU67zZLIqd6K5CPaJw\"\n",
    "# report_id in (21118043);\n",
    "# update bigdata.report_htmls set convert_status = 'pending', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null \n",
    "# where created_at >= \"2018-07-16 14\" and convert_status = 'timeout'\n",
    "# report_id in (21118043)\n",
    "\n",
    "\n",
    "# convert_status=\"timeout\" and created_at >= \"2018-03-07\";\n",
    "# report_id in (19162793);\n",
    "\n",
    "\n",
    "# --delete from bigdata.report_htmls where\n",
    "# report_id in (14771464,15303477,15303576,15303710,15303730,15304315,15304838);\n",
    "\n",
    "# select * from report_htmls where report_type = 9 and convert_status=\"failed\" and error_reason like '%timeout%'\n",
    "\n",
    "# select * from report_htmls where convert_status=\"failed\" and zs_auto_category like '%IPO%' \n",
    "# select * from bigdata.report_htmls where convert_status=\"timeout\" and created_at >= \"2018-03-08\";\n",
    "# select * from report_htmls where convert_status=\"failed\" and error_reason like '%timeout%'\n",
    "# update bigdata.report_htmls set convert_status = 'pending', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null where convert_status=\"failed\" and error_reason like '%timeout%';\n",
    "\n",
    "# select error_reason, count(*) from report_htmls where convert_status=\"failed\" and zs_auto_category like '%IPO%' group by error_reason \n",
    "# update bigdata.report_htmls set convert_status = 'pending', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null where convert_status=\"failed\" and error_reason like '%tool convert done, but fail to postConvert%';\n",
    "\n",
    "# update bigdata.report_htmls set convert_status = 'pending', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null where report_id in (19162793);\n",
    "\n",
    "# update bigdata.report_htmls set convert_status = 'pending', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null where convert_status=\"failed\" and  error_reason is Null;\n",
    "# update bigdata.report_htmls set convert_status = 'pending', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null where convert_status=\"failed\" and  error_reason like '%timeout%';\n",
    "\n",
    "# update bigdata.report_htmls set convert_status = 'timeout', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null \n",
    "# where convert_status=\"pending\" and created_at < \"2018-03-08\";\n",
    "\n",
    "# select * from bigdata.report_htmls where convert_status=\"pending\" and created_at < \"2018-03-01\"\n",
    "\n",
    "# update bigdata.report_htmls set convert_status = 'todo', convert_task_id = null, updated_at = null, html_s3_url = null \n",
    "# where convert_status=\"pending\" and created_at < \"2018-03-01\";\n",
    "\n",
    "# select count(*) from bigdata.report_htmls where convert_status=\"pending\" and created_at < \"2018-03-08\"\n",
    "# select count(*) from bigdata.report_htmls where convert_status=\"pending\" and created_at >= \"2018-03-08\"\n",
    "# select error_reason , count(*) from bigdata.report_htmls where convert_status=\"failed\" group by error_reason\n",
    "# select * from report_htmls where convert_status=\"failed\" and  error_reason like 'tool convert done, but fail to postConvert'\n",
    "# select * from report_htmls where convert_status=\"failed\" and  error_reason like '%timeout%';\n",
    "# select * from bigdata.report_htmls where convert_status=\"failed\" and  error_reason like '%unknown exception%';\n",
    "# select * from bigdata.report_htmls where convert_status=\"failed\" and  error_reason is NULL order by updated_at desc;\n",
    "# select * from bigdata.report_htmls where convert_status=\"failed\" and  error_reason like '%pure image%';\n",
    "# select * from bigdata.report_htmls where convert_status=\"failed\" and  created_at >= \"2018-03-02\" and error_reason = \"\"; \n",
    "# select * from bigdata.report_htmls where created_at >= \"2018-03-02\"\n",
    "\n",
    "# select convert_status, error_reason , count(*) from bigdata.report_htmls group by convert_status, error_reason\n",
    "# select * from bigdata.report_htmls where convert_status=\"failed\" and created_at >= \"2018-03-08\";\n",
    "# select * from bigdata.report_htmls where convert_status=\"failed\" and created_at >= \"2018-03-19\";\n",
    "\n",
    "# select * from bigdata.report_htmls where  created_at >= \"2018-03-19\";\n",
    "# select convert_status, error_reason, count(*) from bigdata.report_htmls where created_at >= \"2018-03-19\"  group by convert_status, error_reason \n",
    "\n",
    "# select * from bigdata.report_htmls where created_at >= \"2018-03-19 0\" and created_at <= \"2018-03-19 15\" and convert_status=\"pending\"\n",
    "\n",
    "\n",
    "# SELECT * FROM bigdata.report_htmls where convert_status = 'failed' order by updated_at desc;\n",
    "\n",
    "# select error_reason , count(*) from bigdata.report_htmls where convert_status=\"failed\" group by error_reason\n",
    "# update bigdata.report_htmls set convert_status = 'pending', convert_task_id = null, updated_at = null, html_s3_url = null \n",
    "# where convert_status=\"failed\" and error_reason is NULL and created_at >= \"2018-02-21\"; \n",
    "\n",
    "# update bigdata.report_htmls set convert_status = 'pending', convert_task_id = null, updated_at = null, html_s3_url = null \n",
    "# where convert_status=\"failed\" and  created_at >= \"2018-03-02\" and error_reason = \"\"; \n",
    "\n",
    "# Qfn6X4roqgPgQyEJALeFBQ\n",
    "# select * from bigdata.report_htmls where convert_task_id=\"Qfn6X4roqgPgQyEJALeFBQ\"\n",
    "\n",
    "# select * from bigdata.report_htmls where convert_status=\"failed\" and  error_reason is Null\n",
    "# 19201060\n",
    "\n",
    "# 19325055\n",
    "# 2102372, 2093101, 2076342, 2059524\n",
    "# select * from bigdata.report_htmls where report_id in (2131018,2130644,2130639,2129227,2102372,2093101,2076342,2024087,1983896,1065787,2059524)\n",
    "# select * from bigdata.report_htmls where report_id in (20180347)\n",
    "\n",
    "# select report_id as id, reportType as report_type, author, title, s3Url as url from research_rpt.dy_res_report where report_id in (2131018,2130644,2130639,2129227,2102372,2093101,2076342,2024087,1983896,1065787,2059524);\n",
    "\n",
    "\n",
    "# update bigdata.report_htmls set convert_status = 'pending', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null \n",
    "# where report_id in (19330640);\n",
    "\n",
    "\n",
    "# update bigdata.report_htmls set convert_status = 'pending', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null \n",
    "# where report_id in (19433817,15580718,19334237,19346881,19346656,20152859,20153778,20161611,20161562);\n",
    "\n",
    "# select * from bigdata.report_htmls where convert_status=\"timeout\" and created_at >= \"2018-03-08\";\n",
    "# select * from bigdata.report_htmls where report_id in (19329824,\n",
    "# 19330679,\n",
    "# 312178,\n",
    "# 364588,\n",
    "# 1562835)\n",
    "# select * from bigdata.report_htmls where report_id in (19433817,19391176)\n",
    "# select * from bigdata.report_htmls where report_id in (19325133,16728614, 2081728,16801197,18765409)\n",
    "# update bigdata.report_htmls set convert_status = 'pending', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null \n",
    "# where report_id in (19329824,\n",
    "# 19330679,\n",
    "# 312178,\n",
    "# 364588,\n",
    "# 1562835);\n",
    "\n",
    "# 16471905\n",
    "\n",
    "# update bigdata.report_htmls set convert_status = 'pending', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null \n",
    "# where report_id in (19325055);\n",
    "\n",
    "# update bigdata.report_htmls set convert_status = 'pending', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null \n",
    "# where report_id in (14967051,14878658,14878825,14878540,14877980,14854241,19325055,19325073,19325176,19326176,19326246,19326043,19326015,19325518,19325487,19325474,19325440,19325436,19325433,19325144,19325922,19325439,19324921,19324888);\n",
    "\n",
    "# select report_id, convert_status, error_reason from bigdata.report_htmls where report_id in \n",
    "# (14967051,14878658,14878825,14878540,14877980,14854241,19325055,19325073,19325176,19326176,19326246,19326043,19326015,19325518,19325487,19325474,19325440,19325436,19325433,19325144,19325922,19325439,19324921,19324888)\n",
    "\n",
    "# select report_id, convert_status, error_reason from bigdata.report_htmls where report_id in (19162793,18891167,19274306,19238219,19239502,19235663,18821284,19162793,18888358,19242081,19279890,19279782,19230088,18891167,19237724,19274306,1564971,19279890,19279782,19300273,19309244,19304864,14792059,14792169)\n",
    "\n",
    "# select * from bigdata.report_htmls where report_id in (19325055)\n",
    "# select * from bigdata.report_htmls where updated_at > \"2018-02-27 12:00:00\" and convert_status = \"failed\" and error_reason like \"%unknown exception%\"\n",
    "\n",
    "# update bigdata.report_htmls set convert_status = 'pending', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null where updated_at > \"2018-02-27 12:00:00\" and convert_status = \"failed\" and error_reason like \"%unknown exception%\";\n",
    "# update bigdata.report_htmls set convert_status = 'pending', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null where report_id in (19321372);\n",
    "#  error_reason like '%unknown exception%';\n",
    "# update bigdata.report_htmls set convert_status = 'pending', error_reason = null, convert_task_id = null, updated_at = null, html_s3_url = null \n",
    "# where convert_status=\"failed\" and  created_at >= \"2018-03-01\";\n",
    "\n",
    "# update bigdata.report_htmls set convert_status = 'pending', convert_task_id = null, updated_at = null, html_s3_url = null \n",
    "# where error_reason like '%tool fail to convert%';\n",
    "\n",
    "\n",
    "# select * from bigdata.announcement_abstract where ID = 38051650;\n",
    "# \t848 \n",
    "# \t5\n",
    "# error while read result file\t1\n",
    "# get s3 file fail\t7842\n",
    "# manual killed\t81\n",
    "# pdf_addr cannot be empty\t2\n",
    "# time out\t6\n",
    "# tool convert done, but fail to postConvert\t477\n",
    "# tool convert done, but with unexpected special code.\t1218\n",
    "# tool fail to convert:Target pdf is pure image\t131539\n",
    "# tool fail to convert:timeout,kill process\t1285\n",
    "# tool fail to convert:translate fail: \t40\n",
    "# tool fail to convert:translate fail: BadData\t554\n",
    "# tool fail to convert:translate fail: Catastrophic: Exception of type /u0027System.OutOfMem\t1\n",
    "# tool fail to convert:translate fail: Could not find file /u0027C://Users//qiong.wu//AppDat\t1\n",
    "# tool fail to convert:translate fail: FileHasCopyProtection\t9\n",
    "# tool fail to convert:translate fail: InternalError\t222\n",
    "# tool fail to convert:[Errno 2] No such file or directory: u'C://process//output//0K09dtLDM\t1\n",
    "# tool fail to convert:[Errno 2] No such file or directory: u'C://process//output//sk_FT3RN-\t1\n",
    "# tool fail to convert:[Errno 2] No such file or directory: u'C://process//output//zH2e3ysM9\t1\n",
    "# tool fail to convert:[Error 145] : u'D:/solid/dest_html\\\\29XcYtkCIRj3j1QvjjdEaA'\t1\n",
    "# tool fail to convert:[Error 145] : u'D:/solid/dest_html\\\\KFhpMRSXP2dU4pus_ouRqQ'\t1\n",
    "# unknown exception: \t5\n",
    "# unknown exception: 'src'\t5\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf D:/solid/origin_pdf/1IQ02ttfeTjKy1\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf D:/solid/origin_pdf/4TWDKouBjkoMZe\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf D:/solid/origin_pdf/7U_26L7ze4BWSI\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf D:/solid/origin_pdf/BGzhEqzs2FtczO\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf D:/solid/origin_pdf/bndxy4TNjEEhjX\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf D:/solid/origin_pdf/gpsvNC4N-QKr4F\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf D:/solid/origin_pdf/JlRsKEsjjReEpQ\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf D:/solid/origin_pdf/knnir1ssCOaU-0\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf D:/solid/origin_pdf/LYq-qowmhG4p2d\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf D:/solid/origin_pdf/qlIBjLtSZH-SOf\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf D:/solid/origin_pdf/sBpaUpQv7u-DRk\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf D:/solid/origin_pdf/t1XNh6wSXjLYbD\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf D:/solid/origin_pdf/UxlLUZnyDzY7J-\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf D:/solid/origin_pdf/VorQMaDf8EANO0\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf D:/solid/origin_pdf/x4b4392zhypbsj\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/0jNDuqFM_iarU6o35ZUg\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/0k4e9r4sY6DZP2r1-cOf\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/1PlGyPzyCsiCTMy-Siur\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/1TdHrV3eHUNkeXJ1pTyv\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/1xervzNSVmrXiI4sJNzW\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/8TzhgtNpZqCmPsuNwfLP\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/A3KPtCpub-XIdawLz-jj\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/BakN5vZn2oSWtpKtw3Iv\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/DoUAB4gClHU-RjhSTVVr\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/ep92R8X0s9Fjp63mSqv9\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/f9RlKsmcwd0rFSDImazy\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/fQBtjV2kuEb6b6SdsYQq\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/GIjZDYWIqmPFWmdT17-4\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/gmEm0GnjWR_z2-zCmihK\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/GOD-_K1zmuF76UhO6npL\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/h8koy4EHZG_AvW57MPMw\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/HJ9qIRAsVvITozRHO1Z3\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/IWnnmjHvBX3NQPFO2Uq0\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/JVairBbRn4A_BDQISLSd\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/kown9aSvpH2MWR4icfUA\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/KWUH7lGQpvQI1mp26ll0\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/KwyDd5w-MBaXmUP17VfK\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/KZCSyhvNRdZuJbtESIrl\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/lx9q_7ssrXV_1UjCjZ9K\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/n2i-iW1ty6Akgben1uEQ\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/NaJkx26wCIzu2qtBjgTs\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/p3AjJMY83r0yQ9IWzGVQ\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/p_aVrvUR8AmI3MTkkeFU\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/rpGUSZXWH-xZ5a_wLO-A\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/TYwUV_UVUf9fdfZOlEGc\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/v-kxWRx9XRcjoLp-vNpr\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/V7Xl071i8dLmvftlPMkC\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/W0scI0VM996t4zyJAqJ7\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/W2pozjdInIPRjvOuSlQe\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/y6P_5PzRZh3Ll337NO8M\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/yF-txdEd70cXwqzHWhkV\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/yYl6Ug4Bu_qI6sFEpLZf\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/Z0qAFip2fufZDI2qj0sF\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/Zqy6_rcSiF-b3qR6fV5o\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/_9xeIaFo4aehdyBfplwG\t1\n",
    "# unknown exception: Command '\"./officeConverter.exe\" pdf Z:/origin_pdf/_pvdDkVaBd0_My_Y4zht\t1\n",
    "# unknown exception: Command '\"./pdf2html.exe\" D:/solid/origin_pdf/-VIvOp07SbB5nR7ElWpTdg.pd\t1\n",
    "# unknown exception: Command '\"./pdf2html.exe\" Z:/origin_pdf/34_uaI2ICr2prGNrnpwLjA.pdf' ret\t1\n",
    "# unknown exception: Command '\"./pdf2html.exe\" Z:/origin_pdf/6ms9WIjs3-2GhLVBJFeh2A.pdf' ret\t1\n",
    "# unknown exception: Command '\"./pdf2html.exe\" Z:/origin_pdf/cWl5zKlGgYs6CU6Z9doIFw.pdf' ret\t1\n",
    "# unknown exception: Command '\"./pdf2html.exe\" Z:/origin_pdf/Fw5v0bbSVr0Y4QCNT3uJzQ.pdf' ret\t1\n",
    "# unknown exception: Command '\"./pdf2html.exe\" Z:/origin_pdf/gfHMg-I70i5B7ODtWjCoDA.pdf' ret\t1\n",
    "# unknown exception: Command '\"./pdf2html.exe\" Z:/origin_pdf/hFma7KXtlfPnbLmOD-sfyA.pdf' ret\t1\n",
    "# unknown exception: Command '\"./pdf2html.exe\" Z:/origin_pdf/iJhAPFgjn8r8PqQBzIG3-A.pdf' ret\t1\n",
    "# unknown exception: Command '\"./pdf2html.exe\" Z:/origin_pdf/jLdU7XtmTG_MG-7a2rxqvA.pdf' ret\t1\n",
    "# unknown exception: Command '\"./pdf2html.exe\" Z:/origin_pdf/KyC7WJ1VbpiYAGX8Z6rxeg.pdf' ret\t1\n",
    "# unknown exception: Command '\"./pdf2html.exe\" Z:/origin_pdf/qHyx9VlaKJQ1R26iNjBffw.pdf' ret\t1\n",
    "# unknown exception: Command '\"./pdf2html.exe\" Z:/origin_pdf/UJIu4WUzca_MeZRf98YX2A.pdf' ret\t1\n",
    "# unknown exception: Command '\"./pdf2html.exe\" Z:/origin_pdf/YrGkODT28O_QoU5-UyGAMA.pdf' ret\t1\n",
    "# unknown exception: Command '\"./pdf2html.exe\" Z:/origin_pdf/ySDruga2t3ehBaF8E5eHvg.pdf' ret\t1\n",
    "# unknown exception: Extra data: line 2 column 1 - line 3 column 11 (char 132 - 213)\t1\n",
    "# unknown exception: [Errno 13] Permission denied: 'Z:/origin_pdf/6dFufaNcegAJ9eT050mjpw.pdf\t1\n",
    "# unknown exception: [Error 1455] \t1\n",
    "# unknown exception: [Error 2] : 'Z:/origin_pdf/XJ0dtS2yawXqLy7C-H0l9Q.pdf_2'\t1\n",
    "# unknown exception: [Error 32] : 'D:/solid/origin_pdf/bmOo_90KCMwlXWLP-EreHw.pdf_2'\t1\n",
    "# unknown exception: [Error 32] : 'D:/solid/origin_pdf/EIx2wqz5tQnPW6tyorN9Bg.pdf_2'\t1\n",
    "# unknown exception: [Error 32] : 'D:/solid/origin_pdf/mRk6TghemH_KGTg51i6OOw.pdf_2'\t1\n",
    "# unknown exception: [Error 32] : 'D:/solid/origin_pdf/TTrlvFzjZ5OWgf01-JK1eg.pdf_2'\t1\n",
    "# unknown exception: [Error 32] : 'D:/solid/origin_pdf/ybgoVzg32_aOwGQZqYT1bw.pdf_2'\t1\n",
    "# unknown exception: [Error 32] : 'D:/solid/origin_pdf/ZxwXv25bnGlliW54_61-OA.pdf_2'\t1\n",
    "# unknown exception: [Error 32] : 'Z:/origin_pdf/BCZJGd5pf_t2CCv7jIm6nQ.pdf_2'\t1\n",
    "# unknown exception: [Error 32] : 'Z:/origin_pdf/p0EUgSNaiHGqNX33jrUkSg.pdf_2'\t1\n",
    "# unknown exception: [Error 32] : 'Z:/origin_pdf/rgwSCq6ZRucR2iZNs62fgg.pdf_2'\t1\n",
    "# unknown exception: [Error 32] : 'Z:/origin_pdf/rOb1TztJMmV37-idzORfkg.pdf_2'\t1\n",
    "# unknown exception: [Error 32] : 'Z:/origin_pdf/rvgYmdTDH5BhtsbwRKzl-A.pdf_2'\t1\n",
    "# 未知\t249\n",
    "\n",
    "# done\t\t3071486\n",
    "# done\t\t96\n",
    "# done\terror while read result file\t148\n",
    "# done\tget s3 file fail\t5860\n",
    "# done\tresult file content is empty\t3\n",
    "# done\ttime out\t837\n",
    "# done\ttool convert done, but fail to postConvert\t91\n",
    "# done\ttool convert done, but with unexpected special code.\t175\n",
    "# done\t未知\t398\n",
    "# failed\t\t1571\n",
    "# failed\t\t94\n",
    "# failed\tdecrypt failed\t6\n",
    "# failed\tempty html\t4\n",
    "# failed\terror while read result file\t1\n",
    "# failed\tget s3 file fail\t9312\n",
    "# failed\tmanual killed\t81\n",
    "# failed\tpdf_addr cannot be empty\t9\n",
    "# failed\tTarget pdf is pure image\t34\n",
    "# failed\ttime out\t8\n",
    "# failed\ttool convert done, but fail to postConvert\t552\n",
    "# failed\ttool convert done, but with unexpected special code.\t1860\n",
    "# failed\ttool fail to convert:Target pdf is pure image\t84\n",
    "# failed\ttool fail to convert:timeout,kill process\t9\n",
    "# failed\ttool fail to convert:translate fail: BadData\t2\n",
    "# failed\ttool fail to convert:translate fail: InternalError\t2\n",
    "# failed\tunknown exception: [Error 32] : 'D:/solid/origin_pdf/C8SFdyQeIW9JkFMfJ66g1w.pdf_2'\t1\n",
    "# failed\t未知\t249\n",
    "# pending\t\t4\n",
    "# timeout\t\t1227\n",
    "# timeout\t\t13\n",
    "# timeout\terror while read result file\t4\n",
    "# timeout\tresult file content is empty\t6\n",
    "# timeout\ttime out\t1714\n",
    "# timeout\ttool convert done, but fail to postConvert\t384\n",
    "# timeout\t未知\t43907\n",
    "# todo\t\t139837\n",
    "# transferred\t\t65\n",
    "# uploaded\t\t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
